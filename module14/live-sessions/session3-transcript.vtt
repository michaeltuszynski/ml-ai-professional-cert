WEBVTT

1
00:00:00.000 --> 00:00:03.229
Mani K: Zoom settings. So it's getting recorded correctly.

2
00:00:03.640 --> 00:00:06.220
Mani K: All right. Yeah. Cool done.

3
00:00:06.800 --> 00:00:07.480
Mani K: Oh.

4
00:00:07.480 --> 00:00:11.470
Manish Goenka: Yeah, while we wait for others to join in. If I can ask you a question,

5
00:00:11.720 --> 00:00:13.180
Mani K: Yeah. Please. Go ahead.

6
00:00:13.530 --> 00:00:16.380
Manish Goenka: Can I set up some time with you this Friday? I think, at 4.

7
00:00:16.960 --> 00:00:17.770
Manish Goenka: That's.

8
00:00:18.466 --> 00:00:19.899
Mani K: It's at 4. 4. Okay?

9
00:00:19.900 --> 00:00:23.929
Manish Goenka: Yeah, I mean, it's flexible. It's up whatever time works for you.

10
00:00:24.160 --> 00:00:24.840
Mani K: Good morning!

11
00:00:24.840 --> 00:00:30.050
Manish Goenka: I wanted to review my project with you, but actually I haven't even submitted it yet. I was just wondering

12
00:00:30.260 --> 00:00:35.480
Manish Goenka: what is the last date for me to submit the proposal, and should I meet with you after that.

13
00:00:36.174 --> 00:00:48.980
Mani K: I mean, you should have submitted by now at least the proposal so just some just go ahead and submit it in in canvas as soon as possible. But I think let's review it in the call. Yeah.

14
00:00:49.380 --> 00:00:50.070
Manish Goenka: Okay.

15
00:00:50.430 --> 00:00:59.280
Mani K: Okay, yeah, yeah. So so you said, let me just check if I have

16
00:00:59.590 --> 00:01:01.929
Mani K: your thing in the calendar.

17
00:01:03.140 --> 00:01:05.625
Manish Goenka: Actually, I set up the meeting online.

18
00:01:06.618 --> 00:01:08.469
Mani K: You use the calendly. Right? Yeah.

19
00:01:08.470 --> 00:01:09.857
Manish Goenka: Yes, there was

20
00:01:10.830 --> 00:01:13.580
Mani K: Yeah, I do see it. Okay, it's at 4, 30. Okay?

21
00:01:13.580 --> 00:01:16.834
Manish Goenka: Oh, 4, 30! How do I find the link for that?

22
00:01:17.564 --> 00:01:22.779
Mani K: It should have a Google meet inside of the invite itself.

23
00:01:22.920 --> 00:01:23.980
Mani K: You don't see.

24
00:01:24.520 --> 00:01:26.649
Manish Goenka: No, no, actually I I should go back and.

25
00:01:27.030 --> 00:01:30.192
Mani K: Yeah, if you can check your calendar, it should be in your

26
00:01:31.110 --> 00:01:36.029
Mani K: it should be in that invite itself. it's a Google meet. Invite.

27
00:01:36.030 --> 00:01:36.870
Manish Goenka: Okay, got it?

28
00:01:38.400 --> 00:01:42.054
Mani K: Alright! Alright! Let's get started. I think we have

29
00:01:43.110 --> 00:01:44.980
Mani K: 3 of you guys here. So

30
00:01:45.579 --> 00:01:59.360
Mani K: I guess I'm not. I'm not sure if this is the final office hour for the year, or if there is one more, I believe, from next week, I think most of us are in, or at least emeritus is on break. I think.

31
00:01:59.940 --> 00:02:01.160
Manish Goenka: What was it? Oh.

32
00:02:01.160 --> 00:02:10.539
Mani K: Yeah, alright, anyway, let's get started. So it's actual.

33
00:02:14.610 --> 00:02:19.989
Mani K: Alright. So before I share this, I think I can put my slide deck on

34
00:02:23.110 --> 00:02:24.300
Mani K: the chat window.

35
00:02:29.530 --> 00:02:30.005
Mani K: Alright.

36
00:02:36.600 --> 00:02:37.300
Mani K: Right?

37
00:02:38.960 --> 00:02:50.399
Mani K: okay. So I think this module I think we are. I think I probably didn't give me 1 min. I think I didn't edit this, I think.

38
00:02:51.750 --> 00:02:52.610
Mani K: yeah.

39
00:02:52.910 --> 00:03:01.910
Mani K: anyway. So I'll I'll edit the agenda slide. I think I forgot to change my agenda in 1 1 min. Anyway. So just

40
00:03:02.609 --> 00:03:11.540
Mani K: brief thing on the capstone. I guess a lot of people have scheduled one on ones with me in the last few weeks, but I think I still have few more to

41
00:03:12.192 --> 00:03:21.560
Mani K: talk to, so if if you can schedule them this week and next, I think it'll be good. I think it'll be great to have, at least your

42
00:03:23.528 --> 00:03:45.769
Mani K: caption sorted out before the break. I think probably you can use the break, and maybe a little bit more time on at least the 1st few steps or 1st few weeks in terms of data, exploration and things like that. So that's the thing. So please schedule the one on one on ones with me at least the 1st one, and then the second meeting can happen

43
00:03:46.130 --> 00:03:53.740
Mani K: somewhere, you know, in between, where you have some meaningful data to. You know, we can review. Okay, so that's the thing.

44
00:03:54.567 --> 00:04:04.772
Mani K: I think the agenda slide. I need to adjust, but I'll skip that, and then I'll adjust it after the call. So I think.

45
00:04:06.148 --> 00:04:19.411
Mani K: in module I think this is Module 12, I believe. I think it was mostly around decision trees. So I thought, I'll just talk about that. Again. A lot of these things are around.

46
00:04:20.308 --> 00:04:32.261
Mani K: the classification, I mean classification and regression. I think any of them. These are like classic machine learning kind of solving problems.

47
00:04:32.740 --> 00:04:52.869
Mani K: and decision trees is pretty straightforward. It's probably the easiest to understand in terms of how it works. It's basically, you know, if you've done any basic economics class, we've gone through some decision tree analysis. So it's basically works the same way. Actually, it starts with a node

48
00:04:52.900 --> 00:05:19.790
Mani K: at every node like you make a decision which way to go and then and then and then you finally go to the best outcome. So here the decision trees are drawn out by the drawn out by the algorithm itself. So so that's that's the easiest way to explain it. You can. We can do both classification and regression kind of problems can be solved with this.

49
00:05:21.230 --> 00:05:35.669
Mani K: it's probably very easy to interpret, because you can visualize the tree and all handles both categorical numerical data. So that's the advantages of it. And it's pretty fast. Actually, okay, so that's the thing.

50
00:05:36.290 --> 00:05:45.600
Mani K: I think the disadvantages of decision trees are like again, if you have a big enough data and too many features and stuff

51
00:05:46.079 --> 00:06:07.800
Mani K: you know, it can easily overfit, because, you know, the branches can keep growing and sometimes that can be like really hard to 1st understand the data of understand how it works. That's 1 thing. I think the other thing is also like, it might be overfitting for the

52
00:06:08.588 --> 00:06:31.601
Mani K: the the training data. Okay? So that's the thing. So whatever training data that you're using is going to be overfitted with that, I think. It's always good to have have that in control, and I think that's the main thing that we'll be looking at in terms of decision trees. But it's a quick and easy way to do solve some problems. Case, that's the thing.

53
00:06:32.190 --> 00:06:56.482
Mani K: any any questions around here around decision trees. So I think I'm gonna go a little bit, dive into the depths, and then maybe I'll I don't have a very specific notebook to share on decision trees. So I'm going to be like, just sharing a simple notebook with the the built in Iris data set, and then show like how it works

54
00:06:57.235 --> 00:07:05.489
Mani K: it pretty much flows the same way for every use case. But I think that's what I'm planning to do in this particular session. Okay.

55
00:07:05.740 --> 00:07:06.480
Mani K: right?

56
00:07:07.728 --> 00:07:09.733
Mani K: Alright. So

57
00:07:11.580 --> 00:07:17.273
Mani K: depending upon how how you begin with on decision trees is going to be around here like

58
00:07:18.662 --> 00:07:33.010
Mani K: once you have your data set, and whether it's a classification or a regression kind of problem, I think you you have like 3 different paths.

59
00:07:33.010 --> 00:07:55.039
Mani K: obviously for regression, you would use the mean squared error. Because that's the right. That's the only way to do it about regression. So we'll be using mean, squared error as a way to. So this is your cost function or your loss function, which we will be optimizing it for. So which would be the mean squared error. So it's the easiest one to understand

60
00:07:55.090 --> 00:08:23.339
Mani K: for the classification we'll be using. You can go with either entropy or this genie impurity. So I'll just describe what it is actually quickly. And it's just like another parameter that you can be that can that can be passed inside the decision tree function to to help with the model fitting part of it. Okay.

61
00:08:24.100 --> 00:08:48.910
Mani K: so entropy, what is entropy? It measures how messy or uncertain the data is okay. So basically, what means is like, if you have high entropy, that means like it has like a lot of I wouldn't say, maybe impurity is not the right way. But I think it's a lot messy. So the thing is when you have, like, a data set

62
00:08:48.910 --> 00:09:10.970
Mani K: that has like just one type of data, like, for example, one class of data, maybe all cats or all apples or all oranges. Then that data is very organized and it has low entropy. So basically, you just have one kind of data set data class. If a group has many types of data like cats, dogs.

63
00:09:11.040 --> 00:09:34.311
Mani K: horses, or like different kinds of apples or different kind of fruits and things like that like now the data is a little bit like messy or jumbled up. So in this case the entropy would be higher. Okay, so that's the thing. So here we are trying to optimize for the

64
00:09:34.820 --> 00:10:03.789
Mani K: And this is how entropy is calculated. It's pretty much like the probability times the logarithmic to the base 2 so, and then the summation of that. So you don't have to remember all these formulas and stuff. But just general. How it works is this, okay? So basically from the from the entropy data, what we are trying to fine tune is like, how low can the entropy can go.

65
00:10:03.790 --> 00:10:21.930
Mani K: or by splitting the data. So at every node where, like, you're trying to take the best decision forward. You're trying to look for reducing this entropy value right? Like that's that's the thing that's what information gain is all about. So so like, for example.

66
00:10:22.445 --> 00:10:42.680
Mani K: Like, if you are. If you have like different kinds of fruits and if you want to like, let's say, split it by color, or like I don't know. Like yellow is bananas, and red is apple. So so that's your that could be one of your, you know, ways of

67
00:10:42.710 --> 00:11:02.540
Mani K: splitting the data, and that would reduce the entropy, because now you're splitting it into bananas and apples and things like that. So that's the thing. Okay? So so that's that's that's how to understand the entropy part of it. Okay, entropy. And the information gain. Okay.

68
00:11:02.640 --> 00:11:03.570
Mani K: is that clear?

69
00:11:05.533 --> 00:11:13.992
Mani K: Genie is another measure of impurity as well. So this one is like,

70
00:11:15.627 --> 00:11:32.262
Mani K: again, same thing. If the if all items are belonging to the same class. Then the this genie value is going to be low, or it's going to be or it's like,

71
00:11:32.700 --> 00:11:49.839
Mani K: less impure. Okay? And if items are mixed, then the impurity is going to be higher. It's also based on probabilities. So like. For example, if a group has 80% apples, 20% oranges. So these are the probabilities. So it's going to be one minus

72
00:11:49.970 --> 00:12:04.409
Mani K: these probabilities, sum of probability squared. And that's going to give you the genie score. So that's the same thing again, you're optimizing here for this genie impurity score. Okay, so that's the thing. So this is another way of

73
00:12:04.770 --> 00:12:24.930
Mani K: optimizing your again. This is your cost, function or loss function. However, you want to call it. So that's that's what you're optimizing it for. Okay, so that's the thing. And then the last one is the mean, squared error. This one we know. I mean, I don't have to explain it much more, but it's it works the same as your regression.

74
00:12:25.030 --> 00:12:27.245
Mani K: Basically, you're trying to

75
00:12:30.319 --> 00:12:39.039
Mani K: the measure, the the errors, and it's the sum of all errors squared mean right? Like says same thing

76
00:12:39.534 --> 00:12:46.070
Mani K: here. Also, you're trying to lower this number, and that's what is going to give you the the best value, because that's thing.

77
00:12:46.579 --> 00:13:06.039
Mani K: So with this like, you can actually like, optimize I mean, like, you can actually draw out a decision tree that can be used for either predicting classes, or it can be for predicting even like numeric variables, which is which is a classic case of regression.

78
00:13:06.671 --> 00:13:15.697
Mani K: Like you might under you might. For classification. It's a little bit easier to understand like, for example, like

79
00:13:18.538 --> 00:13:38.839
Mani K: like I just mentioned, like these, fruit fruit examples, or like which is everything, is like a categorical, variable. It's a lot more easier to draw out the nodes, but the same can be applicable even on the regression, like, for example, like a

80
00:13:38.840 --> 00:14:03.450
Mani K: like a crime, rate prediction, or like a house, house, price, prediction and all. You will have some numerical variables. But you can still have ranges for that, too. Actually. So, for example, if you're having some house price prediction you know, you might have like a Zip code you might have like

81
00:14:03.971 --> 00:14:26.100
Mani K: you might have, like number of bathrooms, square foot things like that. So you can have, like again, decision boundaries on certain things like, for example, a bedroom count greater than 3 or less than 3, right like, or less than, or equal to 3, or compared to zip code, which has, like certain certain number of lists, or like some

82
00:14:26.120 --> 00:14:46.550
Mani K: States have falling under the high price bracket. So it can be like, okay, is this under this bracket and things like that. So you can have like decision notes based on any of them and still can, and that can lead to the prediction of the the house price. So that's that's how it works for a regression. Okay?

83
00:14:48.122 --> 00:15:00.450
Mani K: So the the main thing here is like, you know, if you just like, don't pass any parameters and let the model build out the decision tree on its own. You're gonna

84
00:15:00.600 --> 00:15:21.720
Mani K: you're gonna have a really really large decision tree with with with numerous branches, and that could lead to this is the thing right? Like high accuracy on the training data. And you know, it can also have a very

85
00:15:22.382 --> 00:15:31.984
Mani K: poor effect on the on the test data. So that's the thing. So so this is where like, I think, some some of the techniques like

86
00:15:33.090 --> 00:15:49.480
Mani K: I think what we call as pruning comes into play. And then which is like limit. How do you limit the the branches? And what what other parameters you can tune? And then like. There is also the concept of finding the best

87
00:15:49.480 --> 00:16:07.140
Mani K: set of parameters. Given a certain set of values right? That's just like the hyperparameter tuning. So those are the multiple approaches that you can take to to find the best set of, I guess, like parameters that you can input into the model itself. So that's the thing.

88
00:16:07.564 --> 00:16:22.949
Mani K: Having said that I think this all the other things that we went and my session on. I think it was k nearest neighbor. I think I've spoke a lot about metrics and stuff

89
00:16:23.000 --> 00:16:49.409
Mani K: all of that. All play a factor again. Okay? So any multi class classification problem you will have like imbalances. So you'll have to deal with that, because that can also cause and then cause problems. And then the other one is like, well, I mean, that can cause problems in one way or the other, and the other in terms of whether it's predicting

90
00:16:49.690 --> 00:17:09.759
Mani K: every class in a similar with a similar probability or the other thing that can also happen is like based on your need, whether you want certain like, especially in a binary classification, whether you are more important whether it's more important to actually classify

91
00:17:09.910 --> 00:17:19.750
Mani K: a specific class compared to you. Don't care if if you can deal with some false negatives or false positives depending upon

92
00:17:20.296 --> 00:17:40.296
Mani K: what your business use cases so based on that, like, you can also, you know, do some fine tuning things there. Okay. But anyway, all of those things that we went over in the in the in the k nearest neighbor

93
00:17:41.930 --> 00:17:50.339
Mani K: class over the the metrics. Analysis holds true even here. Okay, so that's the thing.

94
00:17:51.751 --> 00:18:01.658
Mani K: In terms of pruning. I think these are the 3 parameters that we have in that specific you know the psychic library that we are using for

95
00:18:02.160 --> 00:18:28.009
Mani K: decision tree in your notebooks and all. So we have like these 3 things which is like the Max depth, which is limiting the the tree depth like how many layers of the branches you want to go, and then at at a node, or at the very termination point, like, what is the minimum samples that you want to split a node, or to

96
00:18:28.010 --> 00:18:51.600
Mani K: to terminate the point right like. So in that point, like you, you have some of these parameters that you can pass. So you can specifically pass these numbers. That's 1 way to do it. Or you can do a grid search where, like, you can actually say, Give a range of values. And then the function can actually determine, like, what is the right

97
00:18:52.047 --> 00:18:56.640
Mani K: parameters for your data set. So that's the thing. Okay.

98
00:18:56.870 --> 00:19:05.767
Mani K: so that's a, those are pretty high level things. I think it's pretty much follows the same pattern as what you've done before on any

99
00:19:06.896 --> 00:19:18.490
Mani K: any other classification. Things that we've we've learned so far. So nothing much. It's just like, it's just a little easier to understand. decision trees. Okay.

100
00:19:18.890 --> 00:19:23.010
Mani K: there any questions here? 1st of all, before I move forward with any other things.

101
00:19:25.930 --> 00:19:37.619
Mani K: Have have you guys tried out any of the exercises on the or any of the assignments or not. Not assignments, I should say. I think Codey, or practice things

102
00:19:38.030 --> 00:19:40.760
Mani K: on decision trees. Any questions around there.

103
00:19:43.170 --> 00:20:00.580
Ravi Duvvuri: Code your head to do. So yeah, today's submission. Yeah, I'm doing it. Okay? So 1 1 question, I have decision tree and logistics regression in K and N.

104
00:20:01.200 --> 00:20:03.380
Ravi Duvvuri: When we do classification.

105
00:20:03.770 --> 00:20:06.860
Ravi Duvvuri: I I know we have to do all 3 to

106
00:20:07.400 --> 00:20:18.309
Ravi Duvvuri: but logistics is more like a prediction kind of a thing right? And even though it's it's classification. But involves some prediction there

107
00:20:19.262 --> 00:20:32.337
Ravi Duvvuri: between knn and decision tree. Is there a choice like normally like Knn is used predominantly, or decision tree is the predominant thing.

108
00:20:33.460 --> 00:20:37.270
Ravi Duvvuri: just from industry, perspective, like, what's more.

109
00:20:37.870 --> 00:20:39.630
Mani K: Okay.

110
00:20:40.183 --> 00:21:04.229
Mani K: I think the larger the data set, I think it's probably better to go with like Knn and the other classification methods. Because the more the data set is, I think decision tree can become a problem. Okay? So the larger the data set, I think you are better off going with

111
00:21:04.687 --> 00:21:14.699
Mani K: going with other algos like like Kn, or even like Random Forest and Svm, and things like that, you know, there's a lot of other.

112
00:21:14.700 --> 00:21:15.300
Ravi Duvvuri: And.

113
00:21:15.610 --> 00:21:41.980
Mani K: So. So that's that's my. That's why, like, I think, I mentioned initially itself, decision trees are pretty fast and easy to understand, like, 1st of all. So if you have, like smaller data to deal with, and and mostly if you're dealing with like, if you are only dealing with like more categorical variables or primarily categorical variables. I think decision trees will make

114
00:21:42.929 --> 00:21:57.639
Mani K: will make a pretty good model. But if you're playing with lot of like other kind of features. Apart from apart from categorical variables. Then

115
00:21:57.640 --> 00:22:22.260
Mani K: I think that's why decision tree may not be a very great point, because if you you know how the it's the node works with based on decision, so if it's categorical, it's a lot more easier for it to jump those nodes and and eventually get to a point right like. So that is the thing. So it totally depends on the data set and how big the data set is. Also, I think that that's probably the right way to go about it. Okay.

116
00:22:22.260 --> 00:22:44.750
Ravi Duvvuri: Okay? Well, 1. 1 follow up question there like data set. The sheer volume of data set will determine the thing or the unique values of the cat of the features will determine, even if you have like 100,000 rows. If it is like a binary, like one or 2, then this entry will immediately will split it right. Do we need to do that? Our uniqueness.

117
00:22:45.190 --> 00:22:52.340
Mani K: Yeah, so yeah, so when I mean large volume of data set, it also means that there's a lot of variability in the.

118
00:22:52.340 --> 00:22:53.139
Ravi Duvvuri: Okay, got it?

119
00:22:53.140 --> 00:23:03.099
Mani K: Oh, yeah, if you if if you think like, there's only like again, if if it if it falls under like 2 or 3 values then I'm pretty sure like

120
00:23:03.240 --> 00:23:04.990
Mani K: we can do the trick for you. Okay.

121
00:23:04.990 --> 00:23:05.690
Ravi Duvvuri: Thank you.

122
00:23:05.940 --> 00:23:28.098
Mani K: Yeah, on the same subject again. So actually, all of them are probabilistic models. Only. Okay. So I know you made a point like, Hey like, even logistic regression works on probability. Everything is, we are probabilistic models. There's no like

123
00:23:29.531 --> 00:23:36.289
Mani K: deterministic models here. Okay? So everything is probabilistic. Only the only thing is like,

124
00:23:40.510 --> 00:24:01.830
Mani K: like. There will be a lot of variations in terms of performance against these models, because it's it's the way you know, they they all work right like. So that is the thing. So here, like it goes by node by node. So the probability is in this, in this node, like what is the probability in this node? What is the probability? So it goes by that mechanism

125
00:24:01.830 --> 00:24:15.869
Mani K: where? Wherein in Knn it's just like, okay, you already know, like predetermined classes, it's just like, okay, how many neighbors are close to that compared to the. It's like a little bit more simpler, but it's still again a probabilistic model, only so everything is.

126
00:24:15.870 --> 00:24:37.199
Mani K: Svm is also like that. It it divides it into different planes, and then and then depending upon what plane it lies on like, it applies the appropriate classification. So that's the thing. Again, it's all based on probability. So everything is based on probability only at the end, and you can fine tune those

127
00:24:37.310 --> 00:24:50.839
Mani K: again by default. All models are giving you like anything greater than 50 is going to be the other classification in a binary. But you can play with those thresholds, too. Actually.

128
00:24:51.700 --> 00:24:52.490
Ravi Duvvuri: Thank you.

129
00:24:53.850 --> 00:24:58.119
Mani K: Right? Okay. Need access to the slides.

130
00:24:59.700 --> 00:25:02.150
Mani K: Alright, thank you. Let's do it right now.

131
00:25:08.190 --> 00:25:12.730
Mani K: Alright. Okay, cool.

132
00:25:14.370 --> 00:25:25.870
Mani K: Alright. So so I'll just go through the decision tree in terms of how it works in in a notebook environment. Just to give you an idea of

133
00:25:29.655 --> 00:25:46.739
Mani K: you know how how, what functions go in and and and how to read through it. So I'll just do that, and then I think I'll end the call with for like last few minutes for Q&A. Okay, let me share my other screen, and then we can go through this

134
00:26:06.080 --> 00:26:06.850
Mani K: alright

135
00:26:11.700 --> 00:26:33.080
Mani K: again. All right. So this again. This is a very simple notebook. I'm just using the Iris data set. I just wanted to explain the concept. I didn't have a proper notebook for decision tree that I've used before. So apologies for that. But I can maybe create another one with the proper data set with some

136
00:26:34.026 --> 00:26:41.194
Mani K: with couple of examples there. Okay, anyway. So I'm importing all the libraries. It's the same.

137
00:26:44.022 --> 00:26:59.687
Mani K: so this is the decision tree classifier apart from that, like, I think. I installed graph with here, which is one of the libraries to visualize the tree. It depends. I think there are also some built in

138
00:27:02.139 --> 00:27:10.629
Mani K: psychic libraries that can also do this. Actually, I believe it's called Let me check what it is. I think I

139
00:27:13.410 --> 00:27:15.967
Mani K: I think it's called

140
00:27:16.920 --> 00:27:21.040
Mani K: forgot the name of it. I'm just looking up. And

141
00:27:25.660 --> 00:27:32.735
Mani K: okay, is this, okay, I think it's this one. I think it's called Plot 3. I think it's part of sk learn

142
00:27:33.340 --> 00:27:42.489
Mani K: dot 3. I'll just point it out in case if anyone wants. I think this is also one other thing that you can use for plotting the

143
00:27:42.630 --> 00:27:49.750
Mani K: decision tree. Okay, but I'm going to be using graphis. Okay for this one.

144
00:27:50.395 --> 00:28:03.504
Mani K: So here, I'm just loading the iris data set, which is kind of pre-built so, and then splitting the data so and then and then I'm calling the decision tree classifier.

145
00:28:03.930 --> 00:28:20.660
Mani K: I'm using the entropy. You can also pass, genie, I think if you don't pass the parameter by default, I think it picks genie. And then here I'm setting the Max depth alone to 3. I think the remaining things are all are set to

146
00:28:21.297 --> 00:28:49.752
Mani K: the default values. You can select. You can put any random state here. Okay, so that's the thing. So this would fit the training the data. So basically, you're taking the data set from Iris and then splitting it into I'm putting it into 70 30 here. But you can do. However, again, I think, for decision tree. It's always good to have a little larger training data set. Okay? So that's 1 thing, because the because of the fact that it overfits

147
00:28:50.518 --> 00:28:57.510
Mani K: but unless, like, you, you have a pretty good handle on the on the tree depth. Okay?

148
00:28:57.900 --> 00:29:15.120
Mani K: So so this one is just like fitting the data. And then, like this one will give you the the accuracy and the on the training data as well as on the test data. Okay? So so basically, you can see that like for

149
00:29:15.200 --> 00:29:28.820
Mani K: for training. I got 95. And then for test, I got 98 on this one. Okay, obviously, this is just like, the overall accuracy like you may want to do the full I guess, like the

150
00:29:29.060 --> 00:29:44.900
Mani K: the the confusion matrix, and the F, you know the f 1 score all of that metrics. I mean, I'm skipping that part just like giving the main gist of things here. Okay, so that's the thing.

151
00:29:45.040 --> 00:29:53.717
Mani K: And then for for visualizing the tree. Again, you need to export

152
00:29:54.450 --> 00:30:10.274
Mani K: this current graph decision tree visualization in first, st and then you use the graph with to actually plot the data itself. So that's what is being done here. I'm gonna run this one.

153
00:30:11.240 --> 00:30:30.220
Mani K: again. This is that Iris data set which is about the the flower and the petal length and stuff. So you can see that like, you know, this is a node. This is the 1st node, and then, if it is true or false, if it is true, it it determines that this one is this particular class which is Setosa.

154
00:30:30.310 --> 00:30:51.370
Mani K: and then, if it is not. I think it goes to another node, and then it again splits, and then it splits into. So basically, this is the 3 levels right like, this is the first, st second, and then the 3rd decision. So you can see that like pretty clearly like in every node.

155
00:30:51.922 --> 00:31:04.070
Mani K: How it works. You can also see the the entropy number going down as it traverses the the the nodes.

156
00:31:04.100 --> 00:31:13.549
Mani K: and and then you can see. So this is Setosa, and then these, the fine bottom ones, are the other classes, which is

157
00:31:14.039 --> 00:31:22.010
Mani K: this one is Virginica. And then this one is diversity color. So but basically it just goes through

158
00:31:22.760 --> 00:31:25.704
Mani K: 3 layers of decision.

159
00:31:26.660 --> 00:31:51.529
Mani K: nodes to actually determine the differ, the individual classes. Okay, so that's the thing. Again, this is a pretty simple data set again. It can get like more, more and more complex as you go into the as you have like more classes. And and you have, like a lot of features as well. Okay, so that is the thing.

160
00:31:52.071 --> 00:32:03.979
Mani K: And then you can check the the arrows as well. Like, what is the it's basically the one over this one. Okay?

161
00:32:04.140 --> 00:32:19.169
Mani K: So basically, the arrows is just one over that one minus that. Okay, so that's the thing. Now, if you want to do any pruning again, you can actually set your Max depth to any level if you want, like, for example, this one, I didn't do anything

162
00:32:26.053 --> 00:32:27.258
Mani K: you mean, like,

163
00:32:28.197 --> 00:32:37.740
Mani K: pass the data. And that's what I did by passing the training so you can actually just call the model which is like.

164
00:32:41.480 --> 00:32:47.730
Mani K: which is this one clf, dot entropy. And then you can actually use this predict.

165
00:32:48.140 --> 00:32:50.890
Mani K: And then you can pass any data set inside of that.

166
00:32:51.070 --> 00:32:51.760
Mani K: Okay?

167
00:32:52.540 --> 00:32:59.060
Mani K: So you just run this one and then just pass the data inside that. Okay, so that's all it did. Yeah.

168
00:33:01.380 --> 00:33:14.889
Mani K: So in this case, like, I passed the training and the test to make the predictions. But you can. You can pass raw numbers to inside of it, like raw arrays and stuff like whatever format that thing takes in.

169
00:33:15.010 --> 00:33:15.690
Mani K: Okay,

170
00:33:18.610 --> 00:33:38.960
Mani K: right? So that's that. Okay. So the next comes to. You know, like like the pruning which is like you limiting certain things on your own like, for example, here instead of instead of the Max depth of 3 like if I want to prune it to 2, I can set it to 2

171
00:33:39.301 --> 00:33:56.717
Mani K: I can set my you know the the leaf minimum samples for the leaf at 4 for node. I didn't say anything, but if you want, you can play around with this. And then you can check like, what this can give you. Okay, so this is like,

172
00:33:58.010 --> 00:34:14.229
Mani K: hard code pruning. And in this case, like you can see that like it says, I can get an accuracy of 100%. Right? So so you can, you can play around here like this like in terms of pruning it yourself

173
00:34:14.290 --> 00:34:43.709
Mani K: or the other one is to do is using the grid search. Okay? So grid, search like what you can do is like you can pass an array like a list of parameters on which, like you want to. You want to fine tune. For example, I want, like Max, step to be 2, 3, or 5, and then the min sample split to be 2, 5, 10, and then this one is 1, 2, 4, and then you do a grid search. Okay? So that's the thing.

174
00:34:45.130 --> 00:35:06.530
Mani K: so there is a parameter called cross validation. I think this one. It's basically dividing your whatever data with which, like you are fitting the the model with, it'll divide that into, I think, into whatever number you are putting, which is 5 here. So it's

175
00:35:06.530 --> 00:35:25.849
Mani K: going to do 5 passes of it before it determines what is the best parameter. Okay. So that's what cross validation is for. So if you run this, I mean, it's pretty quick. I mean, these. These things are pretty fast, so you can see that like this is the best parameter that based on the list that I provided here.

176
00:35:26.233 --> 00:35:54.560
Mani K: It can say, like, okay, the Max step needs to be 5 samples need to be 4. And then the sample split needs to be at 2. Okay, so that's thing. So with this, like, you can actually like. You can go ahead and fit the your model based on this parameters again, and do one more check on the accuracy. And and then go go ahead and plan and test the

177
00:35:55.010 --> 00:36:03.590
Mani K: the the metrics around that. Okay? So that's the thing that's so that's for parameter tuning with through with grid. Search

178
00:36:05.510 --> 00:36:18.020
Mani K: right? And then the last one is like, I just put a thing around decision tree. So decision tree sorry around regression model. So it's basically the it's using a different.

179
00:36:19.460 --> 00:36:25.343
Mani K: Oh, I think we can. Yeah, we can fine tune. Actually, I mean, like we can.

180
00:36:28.958 --> 00:36:37.850
Mani K: we can actually do another listing on this.

181
00:36:42.710 --> 00:36:48.310
Mani K: I just put a number 9, so you can

182
00:36:48.630 --> 00:36:52.830
Mani K: set the entropy to be this maxed up to be 5

183
00:36:53.732 --> 00:37:00.490
Mani K: which would be a thing, and then example leaf.

184
00:37:27.440 --> 00:37:32.329
Mani K: So so you can do the same thing here, and then you can do the training on here.

185
00:37:52.410 --> 00:37:56.400
Manish Goenka: Money that lead is misspelled. Minimum samples lead.

186
00:38:03.200 --> 00:38:11.950
Mani K: Right? And then you can do the same predictions and stuff. Here again, story. Last part.

187
00:38:19.080 --> 00:38:21.499
Mani K: my, I just didn't find this guy.

188
00:38:22.040 --> 00:38:42.789
Mani K: We can see that. It's now at point 9 6 in terms of accuracy of the training compared to 9 5 before and then. Now, the test is a little bit high. I think this one is almost perfect, which is one. Again, this data set is not. I mean, it's a very simple data set. I think if you play with another larger data set, I think you can.

189
00:38:43.649 --> 00:38:49.629
Mani K: it might be a lot more. You you probably may not get like a test accuracy of 100

190
00:38:51.240 --> 00:38:54.229
Mani K: thanks for correcting that typo.

191
00:38:54.490 --> 00:38:55.150
Mani K: Right?

192
00:38:58.300 --> 00:39:18.359
Mani K: yeah. So that's how you do it again. The main thing here is I don't think the grid search and all. Is that hard like? It's probably a lot easier. I think it's just to play with the different entropy. Sorry, different criteria. I know we didn't try out, Genie, but you can just try Genie, to actually like, for example.

193
00:39:18.805 --> 00:39:26.050
Mani K: in. I think I believe you can pass genie here. I think like this, and you can do a grid search again.

194
00:39:27.525 --> 00:39:35.629
Mani K: And you can see that now for this. you. You are getting a different set of parameters

195
00:39:36.720 --> 00:39:44.280
Mani K: and I think you you can pass this one like it's a leaf is now one, and this is 10

196
00:39:46.930 --> 00:39:55.149
Mani K: and and I. So this is a little bit even more, better, actually, right? So that's the

197
00:39:58.140 --> 00:39:59.540
Mani K: okay, cool.

198
00:40:00.790 --> 00:40:26.338
Mani K: all right? And then I think this one is just like an extension of it, like instead of using the the decision tree classifier, which is what we used here. Here we are using the decision tree regressor. And here, I mean, you can use the same data set or similar data set, like here also you can do the same. So I'm just doing a fit with the Max depth of 3. And

199
00:40:26.780 --> 00:40:43.500
Mani K: and you can calculate what the mean squared error is which would be the the loss or the cost function that you'll be tuning with. Okay? So again, I'm not. I didn't do a full blown exercise on this. But if you have a data set, you can just quickly do the same thing. Yeah, okay.

200
00:40:44.155 --> 00:41:03.325
Mani K: I think the key things to remember, here is like, you know, if if you have a really large data set, probably a large varying data set, I don't think a decision tree is a good thing. If you have a small data set, I think it's better, more categorical variables is better.

201
00:41:03.680 --> 00:41:15.539
Mani K: and then have more data allocate, more for your test data set. That's another thing to keep in mind. Okay.

202
00:41:15.580 --> 00:41:20.669
Mani K: apart from that, like, I think, fine tuning, the

203
00:41:21.100 --> 00:41:29.190
Mani K: doing, the hyperparameter search or using the grid search is a little bit more easier here. So that's all. It is. Okay. So that's those are the key things to keep in mind.

204
00:41:30.140 --> 00:41:33.000
Mani K: But any questions. Yeah.

205
00:41:33.490 --> 00:41:47.590
Mani K: I didn't. I mean, I wasn't planning on sharing this notebook, but if you want I can save it and put it in a Google drive and add it to the slide deck in case if anyone is interested in

206
00:41:48.130 --> 00:41:52.970
Mani K: checking it right?

207
00:41:53.802 --> 00:42:01.070
Mani K: If there aren't any other questions. I I also wanted to. Briefly. Yeah, I'll I'll share it. Yeah.

208
00:42:01.320 --> 00:42:14.750
Mani K: definitely. I wanted to. Also briefly talk about logistic regression. I mean, so primarily, it's used for.

209
00:42:14.880 --> 00:42:17.530
Mani K: Yeah, I'll definitely share the notebook. Okay, yeah.

210
00:42:17.710 --> 00:42:24.349
Mani K: for sure. I'll add it to the slide deck as a link. And then, once it's posted, you should be able to download

211
00:42:24.990 --> 00:42:51.642
Mani K: right for on the concept of logistic regression. Right? So I know it's very easy to understand how it works for a binary classification, because I mean, that's that's the that's the way it works. The logistic regression perfectly works for binary regression. So in the case of if you're using

212
00:42:54.730 --> 00:43:01.049
Mani K: in the case of multi-class, if you're using logistic regression for multi-class problems.

213
00:43:02.450 --> 00:43:16.724
Mani K: I think there are a couple of approaches. I think one is like, you can. it's called one over many. So you can actually like, create multiple models, one for each class.

214
00:43:17.550 --> 00:43:47.209
Mani K: for example, if you are predicting from again for the ease of simplicity, like different fruits, you want to predict like apple. So you create a with binary classification for predicting or with logistic regression, for predicting multi-class problems. You, you will create like multiple models. One is like.

215
00:43:47.830 --> 00:43:52.129
Mani K: Whether it's apple or not. So basically, one over others.

216
00:43:52.220 --> 00:44:08.040
Mani K: Same thing, banana or others, or orange or others things like that. So you are creating a binary classification model for each class as a logistic regression. And then

217
00:44:08.040 --> 00:44:34.290
Mani K: and then you can actually like decide, based on the outcome of each of them, like, what is the what is the desire? What is the predicted class is going to be like? So now the question here is like there could be some conditions where, like, for example, you might have a positive for 2 specific models, for example, sometimes it can predict

218
00:44:34.400 --> 00:44:54.499
Mani K: whether it is an I don't know, like, let's say whether it's an apple or not, it could predict it as a positive sorry. Let's take a different example, whether it's a orange or not it could predict it as an orange, and then, whether it's a grapefruit or not, it can predict it as a positive again for the same

219
00:44:54.924 --> 00:45:11.809
Mani K: fruit. Now, in the sense like, now you have a problem where like? Okay, it predicted positive for both orange and grapefruit, what it is grapefruit and what it is exactly right like. So that's the thing. So you could run into some of these conditions with the

220
00:45:11.980 --> 00:45:16.120
Mani K: one too many are over

221
00:45:16.763 --> 00:45:39.189
Mani K: this kind of like multiple models in in logistic regression for multi class classification. I mean the other. There is another way of doing this which is like. There is a single way of combining all these models into one

222
00:45:39.370 --> 00:45:55.179
Mani K: into a 1. I think it's called softmax softmax softmax regression. I think so. It creates a single model out of these individual models. And I believe it will avoid some of these

223
00:45:57.340 --> 00:46:05.559
Mani K: specific use cases around, like, you know if there, if there is a probability of 2 classes to be

224
00:46:06.040 --> 00:46:31.730
Mani K: highly probable, then like, how do you deal with such use cases? Okay, so that's the thing. So that's something to keep in mind. So I think what I'm trying to get here is like, I don't think logistic is a very good use case for doing multi class problems. I think you're better off using it for binary classification problems.

225
00:46:31.830 --> 00:46:49.540
Mani K: because there are a lot of other models that can do multi-class classifications in a much better way. So that's the thing. But I know we do. We went over multiclass regression. Sorry multiclass logistic regression things in the previous class. So just wanted to point that out.

226
00:46:49.830 --> 00:46:51.250
Mani K: That's all. It is okay.

227
00:46:51.704 --> 00:46:56.470
Mani K: Just how it works and what kind of issues you can deal with and things like that. Yeah.

228
00:46:57.464 --> 00:47:07.449
Mani K: But but I I have done mult. Actually, I've done multi-class regression logistic regression before. And I've used it in production, too.

229
00:47:07.550 --> 00:47:36.019
Mani K: because at that point, like, I think that the data was much simpler to begin with. And I think again, this is where, like, when the data is a lot simpler to deal with, features are less and smaller data set, and all of that, I think you can have simpler models that can work in your favor, I think to be honest, like logistic regression, decision trees all come into this category. Actually. So that's the thing.

230
00:47:39.030 --> 00:48:01.740
Mani K: I think that's pretty much all I had to cover in this particular session. So I'm gonna keep the last 10 min out for Q&A we can talk about any specific topics. Either this this this module, or previous module, or we can also talk about any of your

231
00:48:02.910 --> 00:48:04.990
Mani K: capstone stuff, too, actually.

232
00:48:06.570 --> 00:48:08.420
shashi: Mani. This is Shashi.

233
00:48:08.590 --> 00:48:09.120
Mani K: Yeah.

234
00:48:09.710 --> 00:48:14.955
shashi: Yeah. This last section where you are. Ex explaining the multi class.

235
00:48:16.550 --> 00:48:20.889
shashi: that's a multi class classification, linear regression.

236
00:48:21.050 --> 00:48:25.999
shashi: So like, if I have to work with the majors.

237
00:48:27.860 --> 00:48:33.309
shashi: I'll have some healthy plants and some disease plants, and this is the it can be

238
00:48:33.680 --> 00:48:55.270
shashi: more than the same plant. I mean, for example, tomato. It can have multiple diseases like a leaf curl virus, or a blight, or something like that. So what you mentioned in the last. That is what is very interesting. So how do I go about selecting which option which regression tool to use?

239
00:48:55.810 --> 00:48:57.870
shashi: And how do I pass them in

240
00:48:58.372 --> 00:49:00.730
shashi: like. If I, if I up

241
00:49:01.140 --> 00:49:09.400
shashi: pass a healthy image, it will say, Okay, healthy, and it will say the output as healthy. If I pass a deceased image

242
00:49:09.820 --> 00:49:20.029
shashi: for testing, so it could be one or 2 of the one of the diseases. But it has to go to the next level and then decide right. How do I, Daisy, chain the

243
00:49:20.150 --> 00:49:23.610
shashi: decision? It can, so that it is one of the diseases.

244
00:49:24.510 --> 00:49:30.389
Mani K: So for for you. The everything is based on. It's going to be based on an image right?

245
00:49:30.580 --> 00:49:31.180
shashi: Correct.

246
00:49:31.370 --> 00:49:36.750
Mani K: Yeah. So yeah. So I, yeah. So

247
00:49:36.860 --> 00:50:02.970
Mani K: I think this is where I'm not sure if I haven't dealt with decision trees and or logistic regressions with images. Quite a lot but I think this is where probably you know, some of the other models might play a bigger role because I because of the fact that the way how decision trees are working, or even logistic regression works right? Like, so it's just like

248
00:50:03.903 --> 00:50:25.169
Mani K: If you if you have all of this data in a in a more structured way, like in a like, for example, like if these 3 parameters happen, then it's this disease and this. I don't think you you have some kind of a relationship like that. Then I think.

249
00:50:25.170 --> 00:50:30.610
shashi: I'll be training the I mean, I'll have images for let's say, Dcc, as I'll probably some

250
00:50:30.930 --> 00:50:55.400
shashi: images for disease B, and some healthy, so there will be 3 class of images. One is healthy, and then 2 or 3 kind of diseases. Otherwise, then, what will happen is, I will have to to identify a disease, a, I have to have build a model model and then identify a disease. BI have to build a model and Dcc. And different crop different model like that. It will become so. I was kind of

251
00:50:56.823 --> 00:51:00.159
shashi: thinking about what is the best approach to do that.

252
00:51:00.160 --> 00:51:18.319
Mani K: Yeah, at the end, like when you are, I mean when the image is getting encoded into a binary format, right like. And then you have a label associated with it. Obviously, it knows the patterns right? Like, okay, so this kind of image patterns means that it's

253
00:51:18.730 --> 00:51:48.509
Mani K: it's this disease, or it's whether this is good or something like that. So again, decision trees can work work there. It's just like, you know, the boundaries of those things working with images might be a little bit hard for a model like decision tree. Okay? So that's that's what I'm trying to tell, like, for example, how much variation can the pictures can have to still classify as a specific disease, or whether it's good right.

254
00:51:48.510 --> 00:51:48.910
shashi: Yeah.

255
00:51:48.910 --> 00:52:09.060
Mani K: So that's where like this one could decision, trees might suffer and it can create like a lot of it can create a very complex, you know, decision tree for you, and if you prune it, maybe like you might not get the desired outcome. Okay, so that.

256
00:52:09.060 --> 00:52:38.249
shashi: Yeah, the accuracy will drop, and the moment other than training data. If I give any new image, I think it will fail. If it is the depth is very, very deep. Then it will just be memorizing the images, and if it doesn't match it will keep rejecting, even though we pass a diseased image? Right? So are there better tools? Will you be covering in the coming weeks on? How to classify these images in a better way?

257
00:52:38.590 --> 00:53:01.389
Mani K: I don't think we do a lot of image processing in this class, but I think I maybe I'll I'll make it a note, and maybe, like, we'll go through some examples of images like processing things with images. Okay? So I I believe even in the rest of the session. I don't think we deal with a lot of images, but I'll have to check. Okay, but I'll keep in mind.

258
00:53:01.600 --> 00:53:02.090
shashi: Go ahead.

259
00:53:02.090 --> 00:53:02.670
shashi: Thank you.

260
00:53:03.000 --> 00:53:24.750
Mani K: But so one thing in mind, I think when you are trying to solve a problem, I don't think it's about like, how do I arrive to a point like, Okay, what is the right model to use? I don't think it's you should be asking yourself, okay, these are the set of models I can, or algorithms I can use for this kind of a problem, because

261
00:53:25.012 --> 00:53:40.769
Mani K: you need to try out all of them. Okay, so that is the thing you can. I don't like. So for this, like, you need to know, okay, I need to. Do. Okay, I can do decision tree. I can do logistic I. But I can also do something else right? Like I think that's the thing.

262
00:53:40.770 --> 00:53:45.810
Mani K: If it's about having that clarity a little bit would would be helpful. Okay, yeah.

263
00:53:45.810 --> 00:53:46.360
shashi: Hello!

264
00:53:48.841 --> 00:53:52.938
Mani K: Cool, great question. Yeah, I'm looking forward to your project, actually.

265
00:53:54.404 --> 00:53:58.129
Mani K: especially around that image images.

266
00:53:58.130 --> 00:54:06.578
shashi: Yeah, I'm I'm trying to get the I think by end of this week or just before the break starts, I think I should be landing some

267
00:54:07.250 --> 00:54:19.959
shashi: a couple of thousands of images of some plantations. So yeah, we will. When we do the next after Eda discussion. I will share more information, and we can have a chat on that.

268
00:54:20.920 --> 00:54:42.959
Mani K: Okay, hey, Matt? So you're also doing image classification for your capstone. Okay? Yeah. If you can drop me a note on that I think I think I'll see if I can help on that. And then maybe in one of the future sessions I'll office hours. I think I'll also go over some image classification stuff.

269
00:54:44.400 --> 00:54:45.130
shashi: Thanks.

270
00:54:45.590 --> 00:54:47.800
Mani K: Cool, alright, fantastic.

271
00:54:48.428 --> 00:55:13.100
Mani K: Any other questions or concerns, any anything about projects for anyone? I mean, have you guys started working on it, or like at least like playing with the data set. At least, I think I think that's the most important part at least like playing with the data set doing a lot more lot of you know, Eda stuff.

272
00:55:14.250 --> 00:55:17.400
shashi: I have started some of the this one for the

273
00:55:18.040 --> 00:55:22.237
shashi: yield production. I started looking at the data and see if there is

274
00:55:23.105 --> 00:55:36.504
shashi: good correlation, or if something can be done in the 1st place or not. So that's why I'm keeping 2 options open, I mean for capstone. If the path which 1 1st one I'm taking, if it does not pan out to be a great

275
00:55:36.930 --> 00:55:44.020
shashi: use case for building a model, then probably, at least, I can do a image classifier. So that is how I'm working on.

276
00:55:44.210 --> 00:55:45.359
Mani K: Got it. Got it?

277
00:55:46.014 --> 00:56:11.739
Mani K: I think one other thing I've always mentioned. I think it's not about the you know. Fine tuning a model and getting to the solution. I think that's not what we try to get in the capstone. I think it's the thought process of how you deal with the data set how you will go through the pro your thought process of going through the problem. And that's that's what.

278
00:56:12.140 --> 00:56:17.380
shashi: What business case we are trying to address and the entire, how we apply to the industry and things like that.

279
00:56:17.630 --> 00:56:38.969
Mani K: Like that. I think to that extent, I think, in fact, like, if you if you have the data set, if you have your data set. I think it might be good to do a lot of apart from Eda, like running some. I think I mentioned it in some of the one on ones. But running some hypothesis tests right? I think.

280
00:56:38.970 --> 00:56:41.409
shashi: Yeah, correct. Yeah, that I'll be doing. Yeah.

281
00:56:41.620 --> 00:56:48.990
Mani K: Yeah. So those are things that can get help. You get you more data literate like.

282
00:56:48.990 --> 00:56:49.959
shashi: Yeah, exactly. Yeah.

283
00:56:49.960 --> 00:57:13.410
Mani K: And also it'll remove some of the inherent biases that you might have when you do those tests. Okay? So that's the thing. I think by default. We all bring some biases like, even before we start working on the problem. So that is the thing. So I think if you can do some of those. I think that'll also help you.

284
00:57:13.660 --> 00:57:14.260
shashi: Yeah.

285
00:57:14.950 --> 00:57:15.500
Mani K: Cool.

286
00:57:15.500 --> 00:57:16.090
Mani K: Great, alright!

287
00:57:16.090 --> 00:57:30.219
Mani K: Great! That's pretty much it. I think we're right on time. I'll I'll add this notebook and link into the spread into the presentation, and then I'll have it posted. Okay.

288
00:57:30.510 --> 00:57:44.180
Mani K: all right, everyone have a great holiday break. I think. I think I believe I don't think there's any classes next week, or any modules that gets released next week, so I don't think I'll be seeing you until New Year. So

289
00:57:44.370 --> 00:57:47.130
Mani K: everyone have a great holidays right?

290
00:57:47.130 --> 00:57:50.460
shashi: Thanks, happy New Year to you all, and Happy New Year to everybody. Thanks.

291
00:57:50.460 --> 00:57:52.260
Mani K: Alright, thank you. Bye, bye.

