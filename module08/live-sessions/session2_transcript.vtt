WEBVTT

1
00:00:38.710 --> 00:00:39.820
Ravi Duvvuri: Hey? Manny good morning.

2
00:01:40.160 --> 00:01:41.810
Mani K: About now, can you hear me?

3
00:01:43.540 --> 00:01:44.790
Mani K: Yeah, yeah, I can.

4
00:01:44.790 --> 00:01:45.830
Vivian : Hear you now.

5
00:01:46.170 --> 00:01:49.150
Mani K: Oh, great thanks. I know some reason the

6
00:01:49.190 --> 00:01:56.040
Mani K: Bluetooth headset isn't working alright thanks everyone for joining, and and

7
00:01:57.011 --> 00:01:59.239
Mani K: let's roll into our next session.

8
00:02:00.500 --> 00:02:01.740
Mani K: let me

9
00:02:02.790 --> 00:02:05.260
Mani K: share the slide deck

10
00:02:05.390 --> 00:02:07.080
Mani K: that I'm

11
00:02:07.530 --> 00:02:10.130
Mani K: gonna be working on today.

12
00:02:14.400 --> 00:02:15.200
Mani K: all right.

13
00:02:16.280 --> 00:02:18.669
Mani K: Okay, so today, I'm just gonna

14
00:02:18.830 --> 00:02:25.613
Mani K: just highlight few things around regression. I think that has been the the latest topic around

15
00:02:26.583 --> 00:02:32.789
Mani K: this particular module. So I just, I just wanted to briefly cover

16
00:02:33.378 --> 00:02:38.641
Mani K: a range of things around regression maybe answer any questions. And

17
00:02:39.430 --> 00:02:45.747
Mani K: and maybe in the last 1015Â min, maybe open it up for any questions regarding

18
00:02:47.856 --> 00:03:01.760
Mani K: the the capstone or the project itself. So so we'll keep it like that. And if there are any other questions generally, I think we can, you guys can also up anytime. Okay.

19
00:03:02.493 --> 00:03:06.826
Mani K: so that's going to be the agenda. And

20
00:03:07.800 --> 00:03:12.070
Mani K: let's get it rolling. So again, the

21
00:03:12.582 --> 00:03:33.557
Mani K: I have a lot of notebooks here. I just put in links for these. So I'll generally talk about linear regression. We'll talk about like, why, how simple! Like, what are what are the key things that you can look for in there some of them could be repeatable. I'll I'll try to make it fast but then, like

22
00:03:34.050 --> 00:03:37.849
Mani K: what do you do after you do? A simple regression?

23
00:03:38.500 --> 00:03:54.630
Mani K: and and what are some things that you need to be? Very often you're doing multiple regression. Things like that. We'll just generally talk about and and also talk about some you like real use cases of linear regression. The topic

24
00:03:54.750 --> 00:03:57.809
Mani K: sounds very, very easy or simple,

25
00:03:58.410 --> 00:04:07.330
Mani K: but it has a wide variety of use cases. So I'll generally talk about that as well. So that's that's going to be the thing. Yeah, alright.

26
00:04:08.510 --> 00:04:25.899
Mani K: Okay. So what is linear or what is simple regression? I mean, it's just basically you're trying to find some correlation between 2 variables. One is like an independent, variable, and a dependent, variable. And

27
00:04:26.228 --> 00:04:37.461
Mani K: you try to build. Do a scatter plot between them. Try to see if there is any kind of correlation between them, and if there is correlation you're trying to fit a straight line through it and

28
00:04:40.090 --> 00:04:56.757
Mani K: and if if there is a straight line, if you if you do fit a straight line, is that straight line good enough or not? That's those are the main things that you're checking when when you're doing simple regression. Now in what use cases this can be? In what kind of use cases. It will be like,

29
00:04:57.514 --> 00:05:01.160
Mani K: very powerful. I mean, you're trying to predict something like

30
00:05:02.497 --> 00:05:09.192
Mani K: a numerical value or a continuous value and and

31
00:05:09.770 --> 00:05:23.741
Mani K: the fact that it's it's just like dependent on on one variable makes it a lot more easier to to analyze, to predict but it also comes with its own set of risks, which is like,

32
00:05:25.850 --> 00:05:39.782
Mani K: pretty much. All the variations are dependent on this particular variable. And if there are if there is an actual dependency on multiple other variables, then you're not capturing that, and that's 1 of the downsides of it?

33
00:05:40.440 --> 00:05:41.603
Mani K: How do you

34
00:05:45.611 --> 00:05:47.660
Mani K: Let me just go to psycho.

35
00:05:50.339 --> 00:05:58.470
Mani K: So how do you? How do you figure out like, what is a good fit for a linear regression.

36
00:05:58.480 --> 00:06:07.350
Mani K: Again, the main thing that you've checked is actually around the residuals itself, the properties of the residuals. So what is the residual. It's actually the

37
00:06:07.779 --> 00:06:15.770
Mani K: the difference between the the actual and predicted values. Once you fit a straight line through these scatter plots.

38
00:06:17.080 --> 00:06:18.655
Mani K: you're trying to

39
00:06:20.005 --> 00:06:23.564
Mani K: So there, for any any kind of like

40
00:06:26.520 --> 00:06:32.951
Mani K: like any any kind of machine learning, you actually try to optimize for

41
00:06:34.145 --> 00:06:37.614
Mani K: something called a cost function. So in this case,

42
00:06:38.180 --> 00:06:54.332
Mani K: the best fit line is when the squared of the residuals is kept to a minimum. So we call this always the least squared regression or ordinary, least square regression. These are the typical terms that are used. So you're trying to.

43
00:06:57.251 --> 00:07:13.069
Mani K: So it's basically the average of the squared sum of the regression should be kept to a minimum value. So that's that's that's what we are trying to optimize, or the function that you're using is optimizing your regression for that.

44
00:07:15.249 --> 00:07:20.850
Mani K: so there's a lot of things that you you look for in terms of the residuals itself.

45
00:07:21.280 --> 00:07:24.115
Mani K: like once, once you fit a straight line.

46
00:07:24.820 --> 00:07:34.793
Mani K: there are like 3 or 4 things that you can do actually, to check if if that straight line fit is is something that you can

47
00:07:35.250 --> 00:07:39.119
Mani K: use meaningfully for making predictions. Okay.

48
00:07:41.600 --> 00:07:46.450
Mani K: and based on this analysis, you can actually

49
00:07:46.490 --> 00:07:50.873
Mani K: try to go forward with few other things. Actually,

50
00:07:51.970 --> 00:08:08.526
Mani K: so again, the residuals capture variation. So usually like you do. Some scatter plots of the residuals itself. Try to see how the the residual patterns look like. So this is something that we always look for

51
00:08:09.480 --> 00:08:11.500
Mani K: So the the residual

52
00:08:11.750 --> 00:08:19.769
Mani K: scatter plot should more or less look like a homo homoscholastic plot. So what I mean by that is like

53
00:08:20.651 --> 00:08:30.338
Mani K: the scatter plot should look like a like a flat pipe where, like the scatterplot the residual

54
00:08:31.281 --> 00:08:46.626
Mani K: points are like just scattering around everywhere around 0 but it looks like a flat pipe, and it doesn't look. Doesn't have like a weird fan shaped patterns, or like

55
00:08:48.350 --> 00:08:50.589
Mani K: like kind of a

56
00:08:51.510 --> 00:08:53.758
Mani K: Let me see if I can draw it.

57
00:08:54.760 --> 00:09:09.472
Mani K: So so what you are looking for is like a flat pipe kind of distribution like where every everything is centered around like zeros. What you don't want to have is like

58
00:09:10.690 --> 00:09:14.470
Mani K: like, like some kind of a fan shape pattern like this, or

59
00:09:15.021 --> 00:09:18.939
Mani K: are going around this like this, or or maybe like

60
00:09:19.740 --> 00:09:27.627
Mani K: something like this, like things like that. So those are things that you can easily do once you do the squired

61
00:09:29.570 --> 00:09:31.480
Mani K: regression.

62
00:09:31.640 --> 00:09:33.779
Mani K: Let's see if I can just delete all of that.

63
00:09:33.970 --> 00:09:34.880
Mani K: Alright.

64
00:09:35.960 --> 00:10:01.497
Mani K: so that's 1 thing that we generally do. We also do something called a histogram of the residuals itself. Usually the histogram of the residual should look like nearly normal. That's something that you can also do quickly. So once you do a straight line fit, those are things that immediately you can check, for in terms of the properties of the residuals itself

65
00:10:02.570 --> 00:10:03.215
Mani K: and

66
00:10:07.980 --> 00:10:13.360
Mani K: and then the last thing that we also try to do is

67
00:10:13.819 --> 00:10:26.280
Mani K: check for something called a quantile quantile plot or a cubic plot. Generally it's called so what what it means is like, you're trying to

68
00:10:27.220 --> 00:10:28.862
Mani K: like transpose like

69
00:10:31.980 --> 00:10:36.146
Mani K: a standard Gaussian distribution to a normal distribution like

70
00:10:36.680 --> 00:10:42.313
Mani K: like when you do that like like. For example, here on the on the y-axis, you have a

71
00:10:44.580 --> 00:10:53.396
Mani K: a normal distribution with a mean of mu and a standard deviation of sigma, and then on the x-axis you have,

72
00:10:54.192 --> 00:10:57.408
Mani K: a normal Gaussian distribution, where, like, you have

73
00:10:58.280 --> 00:11:17.352
Mani K: a mean of 0 with a standard deviation of one so, and then if you try to plot all the one sigma points and join them together, you'll you'll get a straight line that that looks like this, that the red

74
00:11:18.726 --> 00:11:26.777
Mani K: the red line that just goes up. So so this is like you are trying to transpose like you're trying to

75
00:11:28.647 --> 00:11:42.472
Mani K: transpose a standard. Dv sorry a normal distribution to a Gaussian distribution. So this is what it would look like for a Qq. Plot like that

76
00:11:43.550 --> 00:11:51.339
Mani K: that. That you're trying to work with using a a standard if a normal distribution. Okay?

77
00:11:51.761 --> 00:12:10.819
Mani K: Like, let's say, like, if you have a skew distribution instead of a normal distribution on the y-axis like, if you plot the standard deviation. Sorry one sigma, 2 sigma standard deviation points. It might look like like this. Okay, so instead of a straight line.

78
00:12:10.890 --> 00:12:13.023
Mani K: So in the case of

79
00:12:15.220 --> 00:12:18.894
Mani K: a proper linear regression, you might have.

80
00:12:21.007 --> 00:12:28.874
Mani K: the distribution of the residuals might look like this on the Y axis. Okay? So what we are trying to do here is like,

81
00:12:31.101 --> 00:12:35.660
Mani K: compare distribute this distribution to the

82
00:12:36.120 --> 00:12:43.399
Mani K: normal Gaussian distribution. So which is the the red line. So we're trying to see like, how much of this

83
00:12:43.750 --> 00:12:46.471
Mani K: Contentil plot looks like to

84
00:12:48.010 --> 00:12:50.542
Mani K: actually, normal standard

85
00:12:53.067 --> 00:13:06.162
Mani K: normal distribution line. So so whether it's hovering over around the red line or not is something that we will always check for. So just this is something called the Qq. Plot, so generally like. If you look at

86
00:13:07.657 --> 00:13:13.470
Mani K: what are the conditions that you look for? After you do a standard

87
00:13:14.880 --> 00:13:24.629
Mani K: simple regression fit is like. Obviously, you look for the scatter plot and see how much the correlation is looking like. The R square value itself.

88
00:13:25.048 --> 00:13:33.512
Mani K: Apart from that, like. You look for the the histogram of the the residuals itself, whether it's looking like nearly normal

89
00:13:34.050 --> 00:13:48.749
Mani K: and then the scatter plot of the residuals like, whether it's like homoscholastic or not, whether it's a flat line flat pipeline and then you also review the Qq plot and then see like whether

90
00:13:49.110 --> 00:14:00.839
Mani K: the distribution of the residuals is pouring around a a standard normal distribution. So that's that's what you're trying to compare. And if

91
00:14:01.030 --> 00:14:18.179
Mani K: if these things kind of like work in favor, then probably you can use the simple regression fit for making predictions. Now, apart from this, like, I think the other big factor that you can also look for is

92
00:14:18.190 --> 00:14:23.837
Mani K: the risk itself, like again, in a simple what is a simple regression? You're trying to

93
00:14:24.230 --> 00:14:25.430
Mani K: prove that

94
00:14:27.100 --> 00:14:30.481
Mani K: the slope is significant in that simple regression. Right? Like

95
00:14:31.978 --> 00:14:33.065
Mani K: it's like,

96
00:14:34.483 --> 00:14:37.056
Mani K: Yeah, you have a

97
00:14:39.191 --> 00:14:51.809
Mani K: output variable y, and you have a b 0, which is your intercept, and then you have a b 1 which might be your slope. So y equal to B 0 plus b 1 x

98
00:14:52.264 --> 00:15:18.885
Mani K: where x is your independent variable. So you're trying to prove that this b 1, which is the slope of the simple regression is actually something other than 0. Okay? So your null hypothesis is that the slope is 0, and you're trying to prove that it's not 0 right like that's your alternate hypothesis. Which which is what you're trying to do here, and whether

99
00:15:21.210 --> 00:15:31.419
Mani K: How significant that is! And then how good of a fit it is is what you're checking by looking at these properties of these residuals. Okay.

100
00:15:38.300 --> 00:15:44.681
Mani K: are there any are there any any questions so far around this? So so

101
00:15:45.890 --> 00:15:55.102
Mani K: So once, once you check all of these, I mean, it's it's pretty There, there are a lot of different ways. You can do linear regression in

102
00:15:56.998 --> 00:15:57.762
Mani K: using

103
00:15:59.340 --> 00:16:14.950
Mani K: there is the line regress function that you can do. There is also like you can you can fit the straight line, using a polynomial function like there's so many, so many variations of

104
00:16:16.440 --> 00:16:32.770
Mani K: options you have in terms of fitting the straight line so I'll show a few a few options here in some of these examples, but I'll just run it through and then, like we can, we can progressively move towards the other forms of regression itself. Yeah.

105
00:16:33.130 --> 00:16:43.139
Mani K: So I'm gonna just quickly share my other screen with my with my notebooks, I'm not going to actually do. Do the notebook. I'll just browse

106
00:16:43.260 --> 00:17:02.555
Mani K: through the notebook and just highlight some important points, because I think I also want to cover the the transformation part of it, and then and then some of the concepts regarding multiple progression as well. So now so in in this particular thing. We are just trying to

107
00:17:03.689 --> 00:17:16.270
Mani K: prove the relationship between the independent variable with one sorry the dependent variable with one independent, variable. Okay, so that's a simple regression concept. So let me just

108
00:17:17.502 --> 00:17:20.760
Mani K: quickly jump into the other notebook.

109
00:17:41.770 --> 00:17:42.700
Mani K: Okay.

110
00:17:44.700 --> 00:17:53.618
Mani K: again, all of these notebooks and the the data set they're all there in the slide deck links that I provided. So

111
00:17:54.050 --> 00:17:55.505
Mani K: so just for

112
00:17:57.870 --> 00:18:05.454
Mani K: just for reference. You can just look up, look up that. So? So this is just like an example of a simple regression.

113
00:18:06.255 --> 00:18:18.218
Mani K: again, I think I mentioned about the Stein and Foster book which is pretty good. So in in that book like, if they give you a lot of

114
00:18:20.535 --> 00:18:28.200
Mani K: like problems at the end of each chapter. And this is one of it's an example from that. Actually, okay.

115
00:18:28.320 --> 00:18:42.020
Mani K: so so I'll just briefly, talk about what this particular notebook is all about. So so what this one is like? The data set is all about like it has

116
00:18:42.120 --> 00:18:48.389
Mani K: R&D expenses of certain companies. As

117
00:18:48.470 --> 00:19:02.730
Mani K: specifically in the semiconductor industry from 2014 the data set looks like this. So you have like 3 columns of data like company name R, and D expenses and total assets.

118
00:19:03.650 --> 00:19:04.550
Mani K: So

119
00:19:05.460 --> 00:19:14.510
Mani K: we're trying to see if there is any relationship between R&D expenses and total assets of the company that that we can establish here. So that's

120
00:19:14.720 --> 00:19:20.888
Mani K: pretty simple to achieve like. So and we want to see like, how good of

121
00:19:22.744 --> 00:19:28.515
Mani K: fit that we can do with a simple regression. So so you want to.

122
00:19:29.783 --> 00:19:43.729
Mani K: you want to see, like, okay, based on how many, how much a company spends on R&D, like, what would be the value of that company so total assets is like kind of the the valuation of the company. If you, if you want to say like, for example, the 1st

123
00:19:43.750 --> 00:19:48.079
Mani K: example in this one Avx corporation.

124
00:19:48.120 --> 00:20:06.230
Mani K: like the R&D expenses, is about like. So this is all in millions. Like all of them. The units are in millions. So so this one, they're spending about 12 million or 11.9 million, and the the the company is valued at about 2 point

125
00:20:07.810 --> 00:20:10.050
Mani K: 4 5 billion. Okay? So that

126
00:20:10.470 --> 00:20:32.600
Mani K: so this is the kind of information we have. Again. Before you can do any sort of analysis. I think a lot of things is around like data cleanup making making sure that the data is transformed to the right data type and things like that. So that's what I'm trying to do here.

127
00:20:32.600 --> 00:20:51.579
Mani K: So we're trying to make sure that this is in individual format, 1st of all. So we're trying to remove some of these dollar signs and then converting it to an individual. So that's the 1st few steps that we're doing here through multiple means.

128
00:20:52.216 --> 00:20:59.800
Mani K: and then, once we have the data we're trying to do a simple scatter plot

129
00:21:01.013 --> 00:21:17.429
Mani K: try to see if or try to establish if there is any kind of a correlation between, do these 2 variables like, for example, here, we're just doing a scatter plot between total assets and currently expenses. Or I think in this.

130
00:21:17.810 --> 00:21:37.305
Mani K: maybe in this particular example, we're trying to see, like, okay, based on the assets of the company, what should be the R&D expenses of that particular company? So we're we want to try to predict that. Okay? So you can see that there is some kind of positive correlation. There's a lot of data points around

131
00:21:38.722 --> 00:21:58.210
Mani K: close to this 100 million mark like, there's a lot of crowding here. But in general there is some kind of a positive correlation. So whether it's a strong, positive correlation, or a weak positive, or a weak negative, weak

132
00:21:58.635 --> 00:22:10.839
Mani K: strong negative or like, there is no correlation at all. That's what you're trying to establish here with the scatter plot. So in this case, like there is some kind of a positive correlation. I wouldn't see whether it's

133
00:22:11.338 --> 00:22:24.249
Mani K: I don't think it's like wide strong, because the data looks very crowded here on one side, but we can see like how how the fit is gonna look like, okay?

134
00:22:24.270 --> 00:22:40.893
Mani K: So in this particular example, I'm going to be using the poly one d as a function so it's like a polynomial function that you can fit. There's some explanation about how how the function works.

135
00:22:41.280 --> 00:23:01.939
Mani K: so basically, you can try to fit any kind of any any order function, a polynomial function. And then you can just feed in the data, and it's going to fit in whatever straight line fit that you want to do here. So in this particular example, what we're going to do is

136
00:23:02.460 --> 00:23:08.590
Mani K: use the lint regress function? So the Linda regress function is part of the

137
00:23:10.096 --> 00:23:13.329
Mani K: this slide by dot stats

138
00:23:14.750 --> 00:23:18.818
Mani K: a library. So we're gonna do use that. And

139
00:23:19.867 --> 00:23:25.032
Mani K: input the X and y, and then it's gonna give you all the

140
00:23:25.897 --> 00:23:42.452
Mani K: parameters of the simple regression which is the the slope. Intercept the r value which is the correlation value, not the correlation squared. It's also going to give you a P value, and the standard error. So you can

141
00:23:43.000 --> 00:23:56.339
Mani K: from this, like you can get the R squared value. And then from the slope and intercept, you can actually like fit a function using the poly one d function. Okay?

142
00:23:57.462 --> 00:23:59.634
Mani K: So with this, like,

143
00:24:01.410 --> 00:24:06.659
Mani K: this is what the r squared value looks like for this particular example like

144
00:24:07.757 --> 00:24:12.772
Mani K: with this total assets and spending the

145
00:24:14.812 --> 00:24:25.401
Mani K: the R&D spend the as per value looks like point 6 3 6, which is like the easiest way to interpret that is like

146
00:24:26.690 --> 00:24:30.198
Mani K: 63.6% of the

147
00:24:31.182 --> 00:24:44.810
Mani K: the independent variable, which is which in this case is like can be the R&D asset can be explained by the total assets is what this is just saying.

148
00:24:45.230 --> 00:24:49.502
Mani K: and then the p-value is pretty low, which means that

149
00:24:50.712 --> 00:24:58.119
Mani K: given any alpha like, either you can choose an alpha of 0 point 0 5 or point 0 1 based on the risk.

150
00:24:58.591 --> 00:25:17.278
Mani K: You can. you can reject the null hypothesis and confirm that the slope is significant, or this particular independent variable has an effect on the dependent variables. That's what it is. And and this is what your equation is going to look like, which is

151
00:25:17.900 --> 00:25:31.060
Mani K: point 0 4 2 is your slope. And then you get an intercept that looks like this, which is 52.9 2. And if you, if you actually plot it out this is how the straight line fit is going to be like, yeah.

152
00:25:31.230 --> 00:25:32.075
Mani K: no.

153
00:25:35.300 --> 00:26:04.159
Mani K: And this is another example of fitting and linear regression using stats model, which is another library. I actually like very much. So it just gives you everything in the resources given in a nice tablour fashion. So you can read through everything in in one shot. So here, here the

154
00:26:05.421 --> 00:26:28.058
Mani K: it's more done like how a machine learning function usually is fit like, for example, you instantiate like this particular function, and then you fit it with the data, and then and then you you print out the results. So it's more like a traditional way of utilizing

155
00:26:28.889 --> 00:26:37.860
Mani K: like an like an Ml algorithm. Okay, so that's all it is doing. Yeah. And once you have all of this, like you can actually try to do

156
00:26:38.190 --> 00:26:50.409
Mani K: the the histogram of the residuals itself. Which is what I'm trying to do here again. You'll have to play with the bin sizes to to to actually like

157
00:26:51.317 --> 00:27:00.070
Mani K: See how the histogram can be visualized in a little bit better fashion, like here. The data looks

158
00:27:00.270 --> 00:27:11.760
Mani K: like you can see that like. Most of the data is closer to the 0 the count, where, like you have 0 but

159
00:27:11.940 --> 00:27:32.652
Mani K: I think if you play with the bin sizes you might be able to get the get a better visual of the the histogram of the the rest itself. So, and then, apart from that, like, you can do a Qq. Plot like for Qq. Plot I'm using this particular

160
00:27:35.062 --> 00:27:38.168
Mani K: the probability plot coming from

161
00:27:38.860 --> 00:27:39.723
Mani K: the

162
00:27:41.580 --> 00:28:11.115
Mani K: the Scipi dot stats so this is what I was talking about like. You know, the red line is going to be your the transposing the normal. So the Gaussian distribution to us to a normal distribution. And then the the blue dots are the ones where, like the histogram, is actually transposed to a Gaussian distribution, and you can see how close it follows

163
00:28:11.850 --> 00:28:28.639
Mani K: to the to the normal distribution transposing so the key here is like, you want the blue dots to be following more or less close to the the red line wherever you see, like

164
00:28:29.487 --> 00:28:41.592
Mani K: lot of separation from that. then you you may. That's like a cause of concern, and you may want to check whether you want to use such

165
00:28:44.589 --> 00:28:51.610
Mani K: linear regression fits in your predictions or not. So that's the thing. So apart from that like.

166
00:28:52.000 --> 00:29:07.299
Mani K: So this is the scatter plot of the the residuals itself, which can be done using a function called recid plot in Cborn. So this is the one that I was mentioning.

167
00:29:08.284 --> 00:29:27.405
Mani K: Is it fan shaped, or something like that, or funnel shaped, or whether it looks like homoscholastic, which is a flat pipe, so you can see that it. It looks like a open fan shaped distribution so these are things that you can look for in a linear regression and try to figure out like

168
00:29:29.480 --> 00:29:37.189
Mani K: whether this is something that you can use for making predictions or not

169
00:29:38.250 --> 00:29:40.217
Mani K: that clear? I mean,

170
00:29:40.850 --> 00:29:43.810
Mani K: I'm just gonna be pausing here for any questions.

171
00:29:49.980 --> 00:29:51.659
Manish Goenka: Hey? Money? I have a question.

172
00:29:52.290 --> 00:29:58.410
Manish Goenka: you know the we go through that videos from the professors. They seem to be using the different libraries

173
00:29:58.763 --> 00:30:10.000
Manish Goenka: or linear regression. And I know you talked about the transformer and the transform as well. And what you're showing here are different ones. So I'm wondering if I'm missing some videos over there, or

174
00:30:10.760 --> 00:30:12.260
Manish Goenka: different ways that I've.

175
00:30:13.170 --> 00:30:28.999
Mani K: You can use any. There's no like like like, I said, like the the simple regression is there in like a lot of libraries, so you can use any of them. if you want to just follow what the professors have been talking about. That's totally fine.

176
00:30:29.240 --> 00:30:48.610
Mani K: I think the key thing here is like the concepts, the understanding of the concepts. Everything is going to remain the same. It's just like which which function or which library you're going to be using is going to be slightly different other than that, like the concepts around residuals. And any of those things are are going to be the same.

177
00:30:49.190 --> 00:30:49.920
Mani K: Okay.

178
00:30:50.410 --> 00:30:54.640
Manish Goenka: And one other question I had was this about null hypothesis, and I was trying to reflect.

179
00:30:54.680 --> 00:31:00.970
Manish Goenka: and I'm I might I might have missed this in one of the earlier modules. Was this covered or are we just supposed to learn more about

180
00:31:01.150 --> 00:31:07.280
Manish Goenka: that concept outside? I've heard this in a few office hours.

181
00:31:07.690 --> 00:31:08.475
Mani K: Okay.

182
00:31:09.260 --> 00:31:09.910
Manish Goenka: And.

183
00:31:10.180 --> 00:31:20.402
Mani K: Yeah. So everything is about creative like, that's the basis with which, like, we

184
00:31:21.690 --> 00:31:27.496
Mani K: you, you're trying to prove like whether whether a certain

185
00:31:28.080 --> 00:31:30.088
Mani K: what should I say? Like

186
00:31:30.710 --> 00:31:59.400
Mani K: So you're assuming that something is not true. And and you're trying to prove it's not the case right like that's the basic for which anything. So for everything. There's going to be like a null hypothesis and an alternate hypothesis. And you're trying to prove that your null hypothesis is false, and that's the best way to do it. And, in fact, like, that's how you should understand concepts. Actually. So that's why I brought it up.

187
00:32:00.020 --> 00:32:04.359
Mani K: In a lot of cases. That's that's how it's going to be around. Yeah.

188
00:32:06.230 --> 00:32:28.500
Mani K: So to answer your question for anything like you will be creating like, even though, like you may not like explicitly say that. Oh, this is my null hypothesis, and this is what I'm trying to prove, but pretty much every concept around statistics is going to have something like this. Okay, so.

189
00:32:28.500 --> 00:32:29.930
Manish Goenka: No, no, absolutely, I think

190
00:32:29.940 --> 00:32:44.259
Manish Goenka: spot on. I was just wondering if I've missed some, and you know I know it's moving fast, so I I might have missed something in the modules, and maybe I don't know for the for the rest of the audience. Here. Have you guys read about the null hypothesis, or I just doing some reading offline

191
00:32:44.290 --> 00:32:46.850
Manish Goenka: to learn some additional concepts here.

192
00:32:48.590 --> 00:32:49.155
Mani K: So,

193
00:32:49.850 --> 00:33:07.090
Mani K: yeah. So yeah. So for simple regression or even multiple regressions, it's a little bit easier to understand, because you're you're trying to fit a straight line so. And and in this particular case your null hypothesis that the slope is 0. Right?

194
00:33:07.090 --> 00:33:28.200
Mani K: So so and then you're trying to prove that it's not 0. And then and then you're also trying to prove that the slope is significant also. Okay, it's not just not 0. But you're also trying to prove that it is. It is different. And it's also significant. Okay, with a certain level of confidence. So that's the thing.

195
00:33:28.510 --> 00:33:29.240
Mani K: Okay?

196
00:33:30.730 --> 00:33:31.770
Mani K: Right?

197
00:33:33.870 --> 00:33:35.790
Mani K: So that's 1 thing.

198
00:33:38.403 --> 00:33:39.056
Mani K: so

199
00:33:39.940 --> 00:33:54.301
Mani K: so simple. So regression is just an exercise. Actually. So if we can explain something with a simple regression train. That's probably the best way to the easiest way to start making predictions now,

200
00:33:54.760 --> 00:33:57.285
Mani K: based on the residual analysis.

201
00:33:57.900 --> 00:34:01.555
Mani K: you can take some different approaches.

202
00:34:02.750 --> 00:34:08.570
Mani K: so the most common approach is actually trying to do some transformations.

203
00:34:11.420 --> 00:34:13.199
Mani K: the form

204
00:34:13.820 --> 00:34:15.360
Mani K: for transformations.

205
00:34:16.040 --> 00:34:17.489
Mani K: Let me.

206
00:34:19.980 --> 00:34:21.929
Mani K: where did my slide deck go?

207
00:34:27.070 --> 00:34:29.260
Mani K: I close past my deck.

208
00:34:44.949 --> 00:34:53.414
Mani K: so if the simple if the simple regression doesn't work, what you can do is actually

209
00:34:56.404 --> 00:35:09.511
Mani K: what you can try to do is like, do transformations. Again, picking a transformation is is a pretty iterative process. But

210
00:35:10.830 --> 00:35:18.314
Mani K: so typical rule of thumb is to use this something called 2 key plot.

211
00:35:18.810 --> 00:35:24.180
Mani K: which which looks like this, and you're trying to compare your scatter plot

212
00:35:24.230 --> 00:35:26.135
Mani K: and see like

213
00:35:27.112 --> 00:35:35.870
Mani K: how the scatter plot looks like. And and you're trying to fit in what quadrant of 2 key plot. It fits. And then you can actually, like, try to.

214
00:35:35.910 --> 00:36:03.556
Mani K: you know, go with that kind of a transformation like, for example, here, like, if my scatter plot looks like this it it looks like it's it's it's in this particular like 4th quarter. And so you can be. You can probably take a log of y or one over y sometimes, like your scatter plot might look like an eye shaped thing like instead of this.

215
00:36:04.920 --> 00:36:05.605
Mani K: so

216
00:36:06.710 --> 00:36:23.829
Mani K: it can, it can look like this with with points everywhere. So if that is the case like you can. You can take this quadrant as well as this quadrant. So in this case, like you can do a log of x and log of y or one over x and one over y,

217
00:36:24.140 --> 00:36:35.043
Mani K: so you can. You can do like a lot of combinations of these like you can transform only one. You can transform both and things like that. And then you you play around here and then

218
00:36:38.120 --> 00:36:50.840
Mani K: and and try to do repeat the same exercise, actually, which is like your transform. Try to fit a straight line again, and then go through the residual analysis one more time.

219
00:36:51.252 --> 00:37:05.337
Mani K: So that's so that's that's pretty much transformations is all about. But it's actually pretty powerful. You'll be surprised with, some of the things that transformations can do actually like.

220
00:37:06.157 --> 00:37:22.840
Mani K: I'll just share one example in the notebook with the data set. And and you can. You can see like how a simple transformation with with with logs can actually make a

221
00:37:23.312 --> 00:37:26.270
Mani K: make an impact in simple progression. Okay.

222
00:37:26.320 --> 00:37:29.528
Mani K: so let me just jump into that notebook one more time

223
00:37:30.100 --> 00:37:33.739
Mani K: and you can see the power of transformation.

224
00:37:48.150 --> 00:37:49.460
Mani K: Oh.

225
00:37:59.400 --> 00:38:00.330
Mani K: let's see

226
00:38:00.940 --> 00:38:03.390
Mani K: way too many windows in my

227
00:38:13.100 --> 00:38:13.790
Mani K: alright.

228
00:38:16.870 --> 00:38:22.670
Mani K: alright! So so this is another data set again.

229
00:38:24.340 --> 00:38:27.180
Mani K: So I have a data set of

230
00:38:30.080 --> 00:38:35.879
Mani K: the country with Gdp values, and the

231
00:38:37.872 --> 00:38:53.987
Mani K: the carbon dioxide emissions. So we're trying to see if there is any kind of a a linear relationship between the the country size and and the amount of emissions the country is having. Okay? So

232
00:38:55.090 --> 00:38:59.216
Mani K: so if you look at it. So this is what the

233
00:39:02.421 --> 00:39:24.928
Mani K: this is what your scatter plot looks like between these 2 variables that we have. So we have Gdp in the X and Co. 2 emissions in the Y, and your scatter plot looks like this. And if you do a straight line this is how the fit might look like

234
00:39:25.470 --> 00:39:34.540
Mani K: and and I'm not doing any residual analysis but feel free to based on the data

235
00:39:34.650 --> 00:39:42.522
Mani K: setup. Provided if you want, you can do the residual analysis on this. But you can try to do

236
00:39:44.538 --> 00:40:08.469
Mani K: use the 2 key plot from this. And and I'm just trying out log transformation. So I'm trying to do log of the y first, st which is the Co. 2 emissions, and then I'm trying to do a straight line fit. And this is how it looks like. And then I try to do the log of both of them.

237
00:40:08.520 --> 00:40:23.593
Mani K: and you can see how just by doing a log log transformation. Now, your data looks like this. So this is the scatter plot of the log log transformation. And then you're trying to fit a straight line

238
00:40:24.550 --> 00:40:27.070
Mani K: and then you can see, like

239
00:40:27.520 --> 00:40:33.359
Mani K: the power of transformations in this one particular plot itself, like, you know, we came from

240
00:40:33.796 --> 00:40:41.513
Mani K: this kind of scatter plot with a with a straight line that fits like this to

241
00:40:42.680 --> 00:40:47.088
Mani K: a scatterplot that now looks like this with a different

242
00:40:48.140 --> 00:40:54.439
Mani K: with with the transformed values. And you can see that like, it's a lot more

243
00:40:55.960 --> 00:41:17.760
Mani K: usable in terms of the straight line the fit itself. Okay, so that's the thing obviously, like, after this, you can do all the remaining rest analysis like, just because this looks good. Doesn't mean that like you can. it's good enough to start using the

244
00:41:19.590 --> 00:41:33.239
Mani K: the the regression fit line. You still want to do the residual analysis and try to make sure that, like all the properties of the residuals, looks like it. So this is the scatter plot of the residuals

245
00:41:36.210 --> 00:41:41.020
Mani K: of the original one, I think, and this is for the log log fit, which looks pretty good

246
00:41:41.740 --> 00:41:47.310
Mani K: then. This is the histogram of the residuals

247
00:41:47.380 --> 00:41:56.560
Mani K: for the log log fit and then this is the the the Qq. Plot that I just mentioned about with the residue, else

248
00:41:57.051 --> 00:42:15.138
Mani K: so so those are the few things that you can do. I think so. Log log is quite powerful a lot of cases people try to do it like inverse transformation is also quite powerful.

249
00:42:17.490 --> 00:42:32.039
Mani K: I haven't seen like a lot of squire transformations. But but those are. Those are also some other common forms of things that you can do actually here. But just

250
00:42:32.910 --> 00:42:37.859
Mani K: So I think the key thing here is like. I mean, you don't have to drop

251
00:42:38.276 --> 00:42:59.693
Mani K: the simple regression just because it wasn't a good fit to begin with. But just playing around, transforming your x, and y can actually like like can give like, really good. Actually. So that's the thing. So that's that's the point I'm trying to make here.

252
00:43:00.810 --> 00:43:28.690
Mani K: and if a simple regression doesn't fit the bill, then actually, you can try to add multiple independent variables. Now, what I mean by that like is, instead of one x like you can have like x 1 x 2 x 3 kind of a thing. And and you are trying to do a multiple multiple regression. So which is like, you're explaining why, with a combination of independent variables. Okay, so that's the thing.

253
00:43:30.620 --> 00:43:36.386
Mani K: so so what are some of the use cases of? The

254
00:43:37.735 --> 00:43:39.405
Mani K: simple regression?

255
00:43:40.270 --> 00:43:46.342
Mani K: I think. In the financial side of things like, I think you might have heard about.

256
00:43:47.239 --> 00:43:51.670
Mani K: a Ca capital asset pricing model, which is the cap and model

257
00:43:52.070 --> 00:43:54.898
Mani K: which is which is used to

258
00:43:55.420 --> 00:43:57.600
Mani K: kind of predict the

259
00:43:59.410 --> 00:44:17.678
Mani K: the returns of any particular asset, or like a stock based on whatever the Mark Market is returning. So it's called a cap and model, and if you look at it, the underlying concept there is just a simple regression. Only like, if you go to any

260
00:44:18.440 --> 00:44:43.854
Mani K: like like Yahoo finance and search for any any particular stocks. details like they will put a value for Beta, which is nothing but the the slope of the capital model. Okay, so that's that's so it's pretty much like there, and simple regression is there in like a lot of these places?

261
00:44:44.410 --> 00:44:48.694
Mani K: the other one simple regression is quite

262
00:44:51.070 --> 00:44:55.897
Mani K: popularly used is around simple pricing. Where like

263
00:44:57.101 --> 00:44:59.828
Mani K: you're trying to do

264
00:45:00.780 --> 00:45:07.469
Mani K: I'll probably go over an example later down the line, or I can even share some notebooks around it like.

265
00:45:08.002 --> 00:45:13.747
Mani K: so you're trying to fit a straight line based on

266
00:45:15.070 --> 00:45:28.525
Mani K: some supply and demand data for doing some simple pricing concepts. And you can play around there for different strategies like, in terms of whether you want to.

267
00:45:28.960 --> 00:45:51.473
Mani K: like, if you want to maximize profit like what would be what is the price you need to set or like? If you're trying to maximize revenue? What kind of pricing strategy you need to set all of that is just based on simple regression itself. So basically, you can do like price for simple pricing. You can do actually, like

268
00:45:52.228 --> 00:46:11.272
Mani K: analysis, using simple regression and and use some of those concepts. Actually. So that's things. So that's 1 other use case that I can think of. For for some progression, the other thing is, when you're using transformation. Again, this is for

269
00:46:12.861 --> 00:46:36.490
Mani K: sharing the the the findings or results, or taking out the messaging part you might have to do the reverse transformation. To to explain it in a much better way, because explaining it in log and things like that might be a bit harder. So to understand. So that's the thing. So always just

270
00:46:36.910 --> 00:46:49.179
Mani K: try to keep in mind that like once you've transformed. It's in a different scale. So although log transformation works quite well. Because, like, yeah, I think the

271
00:46:49.763 --> 00:46:56.526
Mani K: you log transformations you can actually talk about in percentages slot easily

272
00:46:57.841 --> 00:47:08.739
Mani K: so you may not have to transform back to its original scale. But that's it's something to keep in mind, too. Okay, so that's the thing.

273
00:47:09.294 --> 00:47:11.111
Mani K: So this is about

274
00:47:14.387 --> 00:47:31.919
Mani K: transformation. Now, if this doesn't fit, then I think you can do to multiple regression. Now, I'm not going to explain multiple regression in depth. I think the key thing in multiple regression is the you can fall into a trap of what we call as

275
00:47:32.447 --> 00:47:44.430
Mani K: covariance which is like, let's say, like you have, you keep on adding, like, multiple independent variables, like, let's say, like you have, like

276
00:47:44.450 --> 00:47:49.575
Mani K: 5 or 6 independent variables. Now, the thing is, there could be

277
00:47:50.130 --> 00:47:52.349
Mani K: a correlation between

278
00:47:52.370 --> 00:47:54.440
Mani K: the independent variables

279
00:47:56.370 --> 00:48:13.589
Mani K: which leads to this thing called covariance, like, for example, like you have x 1 x 2 x 3 x 4, maybe x 2 and x 3 are linearly correlated. You might miss that and this could lead to

280
00:48:19.550 --> 00:48:33.910
Mani K: this could lead to a false sense of hope that the particle regression is fitting well, but then, like, it may not be. In real use case when you start predicting with them. Okay, so that's the thing.

281
00:48:34.440 --> 00:48:35.940
Mani K: Let me just

282
00:48:36.020 --> 00:48:39.420
Mani K: share my slide deck back. And then

283
00:48:39.470 --> 00:48:41.370
Mani K: I'll just talk about what that is.

284
00:49:04.720 --> 00:49:05.530
Mani K: Okay?

285
00:49:07.970 --> 00:49:08.859
Mani K: Okay,

286
00:49:11.320 --> 00:49:29.906
Mani K: So yeah. So the thing that I was talking about multiple regression is linearity effects or the covariance effect like, if 2 or more exponentary or independent variables are linearly correlated. So so this could lead to

287
00:49:31.097 --> 00:49:49.080
Mani K: you might get a decent enough fit like with the with the r squared value like being pretty high but like it could it could have some issues with your predictions itself, like later down the line if you're using it for making predictions.

288
00:49:49.080 --> 00:50:04.853
Mani K: So how? In order to understand this like there is something called variance inflation factor. So once you fit a straight line, this is also something that you can in addition, do to all the other things that

289
00:50:05.220 --> 00:50:29.783
Mani K: I just talked about regarding the properties of the residuals, the Qq plot, and all of that right like. So you will continue to do the same thing even for multiple regression. But in addition to this, like, there is another thing that we may want to check, which is called the variance variance inflation factor, which is kind of a proxy to understand whether there is collinearity between the independent variables. Okay?

290
00:50:30.650 --> 00:50:34.733
Mani K: so how does this work? So let's say, like,

291
00:50:36.270 --> 00:50:55.200
Mani K: let's say, like we have. Let's say, this is the example that I have. Like, I'm trying to predict, like a percentage change, estimated percentage change in a stock prices returns. For example, in this particular example, specifically, Sony's

292
00:50:55.525 --> 00:51:15.684
Mani K: and we are trying to fit a stray multiple regression with these 4 indexes. Okay? So I have the market. I have the Dow Jones Index, and then there is 2 other indexes which is small, big and high, low. And let's say we fit a stray multiple regression. And these are the slopes for each of them.

293
00:51:16.930 --> 00:51:36.529
Mani K: to check for collinearity. What I mean by collinearity is like if there is kind of linear correlation between any of these variables. So that's what we're trying to establish here. So between market and and dow, or between market and small, big or market and high, low.

294
00:51:36.530 --> 00:52:04.790
Mani K: So how do we do that? So basically, the steps are these actually, so you regress one independent variable over the other. So in this case, like, we have 4 independent variables. So we'll do 4 regressions. So basically, market versus the other 3, Dow versus the other 3 small big versus the other 3 high, low versus the other 3. So you do like 4 regressions you get the R squared value from that. And then you

295
00:52:07.842 --> 00:52:14.339
Mani K: you do a 1 over one minus r square in inflation factor. So once you have that

296
00:52:18.258 --> 00:52:34.669
Mani K: numbers are close enough like, for example, in this particular example of this sony sony stock prediction. You can see that these are the the variance inflation factor. Numbers like, I have 9.8 3 here. This one is 9.0 2

297
00:52:34.670 --> 00:52:52.339
Mani K: 1.5 6 1.2 5. So you're trying to see like if any of them are greater than 5, and and if they, if they are close enough like, for example, market and Dow. Both are close to similar variance inflation factors.

298
00:52:52.610 --> 00:53:08.125
Mani K: and they are greater than 5. So maybe there is a lot of collinearity between market and Dow. So in this particular example, I think we have a decision to make, whether

299
00:53:10.160 --> 00:53:12.810
Mani K: to use one of them

300
00:53:13.530 --> 00:53:34.035
Mani K: or you, or what we can also do is like, do an average of market and dow, or use a difference of market endow and things like that. So basically, you want to remove the collinearity factor between market and Dow. So that's the thing. So so this is a common

301
00:53:35.130 --> 00:53:36.910
Mani K: issue that have

302
00:53:38.340 --> 00:53:39.020
Mani K: this

303
00:53:40.040 --> 00:53:43.099
Mani K: when you keep on

304
00:53:43.790 --> 00:53:45.609
Mani K: adding like

305
00:53:47.528 --> 00:53:48.744
Mani K: the so-called

306
00:53:51.436 --> 00:54:10.419
Mani K: r squared value to to make sure that there is a good enough fit for the straight line. Okay, so that's the thing. So this is something that just make sure that like when you're doing multiple regression, you. You keep an eye on and play along there. So that's that's what it is. Yeah.

307
00:54:11.444 --> 00:54:26.430
Mani K: so those those are the key main things I would say, that you need to be aware of. Like, let me just end with a notebook that explains this particular data set. I'm just talking about?

308
00:54:37.110 --> 00:54:37.790
Mani K: okay.

309
00:54:39.230 --> 00:54:44.556
Mani K: so so this is the same example. I just talked about

310
00:54:45.430 --> 00:54:56.343
Mani K: so so we have the percentage change. And sony stock price. And then we have data on Market Dow. Small week, high, low

311
00:54:57.320 --> 00:54:59.093
Mani K: like you can do.

312
00:55:01.330 --> 00:55:27.619
Mani K: multiple regression using the linear regression function pretty straightforward. So you don't have to do any of those analysis. So I mean, you don't have to do any complex things. So you can use the linear regression function from from a lot of these things. You look at the R squared value. But when you are doing multiple regression, I think you. You look for something called the adjusted R squared because you want to

313
00:55:28.063 --> 00:55:37.796
Mani K: if you keep on adding independent variables like obviously the R squared value will keep going up. So this one

314
00:55:38.220 --> 00:55:58.460
Mani K: it's always good to look at the adjusted R squared value for when you're doing like multiple regression. But let me skip the the main thing when go go to the the Vif thing. So here I'm I'm just doing the variance inflation factor in a manual way to show these numbers. So here

315
00:55:58.879 --> 00:56:06.660
Mani K: I'm just printing the values for those in a quick way. But you. You also have, like,

316
00:56:09.678 --> 00:56:28.391
Mani K: dedicated functions for these 2. Actually, for example, in stats model, there is a variance inflation factor function that you can just bring in, and it can just print the values for it. So once you have that like in this particular example, I'm just like,

317
00:56:29.224 --> 00:56:46.600
Mani K: taking an action and so instead of using both market and dow separately, I'm just doing an average of that. And then I'm just checking the if the vif value after that goes down or not. Okay. So that's the thing.

318
00:56:46.630 --> 00:56:48.904
Mani K: So I 1st did the

319
00:56:49.560 --> 00:56:56.409
Mani K: I 1st used all 4 of them and then I found that the vif value was

320
00:56:57.187 --> 00:57:09.632
Mani K: greater than 5 and close enough for these 2. So and then I made a decision to use an average of market and dow and then and then. Now, my vif is

321
00:57:10.280 --> 00:57:22.509
Mani K: pretty much looking more or less. Okay and and I don't have any collinearity issues based on this. And then and then you can still proceed with the rest of the analysis to see?

322
00:57:23.007 --> 00:57:35.409
Mani K: If if this could be used as a good multiple regression function to explain this particular output variable, because that's all it is about, okay.

323
00:57:35.500 --> 00:58:02.610
Mani K: So, yeah. So those those are the key things that I just wanted to go over. So just to recap. So you can start with a simple regression. Do some basic analysis around the residuals to see if if it can be used as is if it doesn't work, you can do some simple transformations, and then redo the same exercise playing with residuals to see if that fits the bill.

324
00:58:02.972 --> 00:58:30.167
Mani K: If not, you can actually like add multiple independent variables. And try to see if that fits the bill or not. Only thing is when you're doing that like you just watch out for collinearity effect. We haven't talked about this, but you can also do transformations with multiple independent variables as well. So that's that's another thing. So you can do a lot of things around regression itself to still

325
00:58:31.488 --> 00:58:48.451
Mani K: try to explain. An independent variable with multiple independent variables. The good thing about regression is, it's it's a very simple thing to explain. if if something can be fit with regressions, I think

326
00:58:49.712 --> 00:59:08.169
Mani K: and you can explain it. I think making predictions is a lot more easier. And it's it's less compute, intensive to actually. So those are things. But they are quite powerful, actually. And you'll be surprised in in how many places regressions are still being used.

327
00:59:09.020 --> 00:59:10.089
Mani K: So that's thing.

328
00:59:10.200 --> 00:59:12.069
Mani K: So I just see top here.

329
00:59:12.890 --> 00:59:13.550
Mani K: Yeah.

330
00:59:16.640 --> 00:59:24.880
Vivian : Oh, yes. And so when you found those 2 variables that have high collinearity.

331
00:59:25.420 --> 00:59:29.499
Vivian : you take an average of them. Can you just pick one of them instead of.

332
00:59:29.500 --> 00:59:30.830
Mani K: You can also do that.

333
00:59:30.940 --> 00:59:37.219
Mani K: Yeah, you can do. You can do that approach, too. You can just drop one of them and use the other one. Yeah.

334
00:59:37.220 --> 00:59:49.609
Vivian : Usually, what is your criteria to pick one of them? One of them, the Vif value, is 9.8, the other one is 9.0. Would you say that you would want to pick the lower Vi value.

335
00:59:49.610 --> 00:59:55.956
Mani K: So again here, like, I think, I would. So instead of using the

336
00:59:56.500 --> 01:00:21.480
Mani K: the I guess the so-called data analysis had, I think the better way to approach is putting the subject matter. Expert hat like you you can. If if you understand what market is like S. And P. 500 or anything like that. And then Dow Jones like, is that like a better representative of the market like? You know, I think you

337
01:00:21.480 --> 01:00:26.823
Mani K: you should wear that hat more than like whether to drop one or the average.

338
01:00:27.558 --> 01:00:39.979
Mani K: these are some some simple approaches, but I think I would rather put this Sme hat and then try to figure out like is one better over the other, or should I use both of that right.

339
01:00:39.980 --> 01:00:42.089
Vivian : Basically use domain knowledge to really.

340
01:00:42.090 --> 01:00:47.139
Mani K: Yeah, to use the domain knowledge to to understand what to do there. Okay, yeah.

341
01:00:47.320 --> 01:00:47.850
Mani K: okay.

342
01:00:47.850 --> 01:01:08.429
Vivian : Great? And can I? Sorry? One more question is like, Do you have, like general like resources for the rule, for transformation like, how do when we look at the shape of the scatter plot like? When do we? If it's like parabolic, we do squared? And then when do we do square root. And

343
01:01:08.430 --> 01:01:19.650
Vivian : if I believe there's a general rule, right when you look at the shape like, if it's both our or inward. But do you have a resource that way. We could kind of go and say, well

344
01:01:19.650 --> 01:01:27.400
Vivian : for this type. For this shape, this type of transformation methodology is the best for this type of

345
01:01:28.000 --> 01:01:45.874
Mani K: So so again. So that's what like, it's more guidelines, only. So the best part, the best thing, is like you can iteratively do this, or in a quick fashion so you can start with what these guidelines give you

346
01:01:46.290 --> 01:01:46.690
Vivian : But.

347
01:01:46.690 --> 01:02:09.359
Mani K: Sometimes like it doesn't always. This is where the science and art kind of mixes. So for your data set, it might be slightly different from the guideline itself. Okay, so that's the thing. So I just mentioned about the the 2 key plot, right? Like, I think that is just giving you a guideline to start with. Okay, you can begin with this, and then you can.

348
01:02:09.360 --> 01:02:22.509
Mani K: The other thing is like you don't have to transform everything in one shot. Okay, I think you can slowly add complexity. So I think keeping it simple is much better. So you start with one and then

349
01:02:22.874 --> 01:02:32.360
Mani K: and then you transform other multiple variables and then try to see like which one fits better. So that's the thing. So that.

350
01:02:32.380 --> 01:02:47.920
Mani K: I I think the answer here is like, take an iterative approach and then start from one, and then go from go in steps to to see if it improves or or degrades your performance. And things like that. Okay.

351
01:02:47.920 --> 01:02:49.370
Vivian : Okay. Great. Thank you.

352
01:02:49.510 --> 01:02:50.149
Mani K: Go ahead.

353
01:02:52.450 --> 01:03:08.299
Mani K: alright, I think I think that's pretty much all I had for this session again. I think we're pretty close to getting into picking the topics for your projects to. Actually, I think we'll probably have some sessions around those pretty soon. I think if you haven't

354
01:03:09.180 --> 01:03:20.600
Mani K: thought about it yet. I think it's it's time to spend some time, and trying to figure out the plans around that as well to actually so, if there are any questions

355
01:03:21.075 --> 01:03:26.859
Mani K: regarding your projects, I think feel free to reach out to me as well on that. If that's those are things.

356
01:03:26.860 --> 01:03:28.719
Matt Lee: Hi, Manny, I do have a quick question if you don't mind.

357
01:03:28.720 --> 01:03:29.740
Mani K: Yeah, I'll go for it.

358
01:03:30.420 --> 01:03:40.059
Matt Lee: Yeah. So it's kind of what I typed there. So I thought, like, the whole point of Pca is to find like linear relationships right? And then you can quantify those, order them and get rid of the ones that

359
01:03:40.320 --> 01:03:44.470
Matt Lee: you know you don't need. Right? So can you apply that for this case, and

360
01:03:44.620 --> 01:03:48.179
Matt Lee: that'll give you education. Educated way to get rid of columns right.

361
01:03:48.750 --> 01:04:03.198
Mani K: Yeah, yeah, you can actually. So you you like, like, what variables to use? What? So those those kind of things you can do and and then use it in your analysis. Okay? So

362
01:04:03.938 --> 01:04:30.990
Mani K: you can. Again, it. It totally depends on the data set that you have like, if you have too much of data to play with, then I think the Pca. Comes into play like, another thing is like, if you have, like very few data to begin with. And this is what I talked about in about the data generation process understanding that. And maybe you may want to track few other things, or how to

363
01:04:31.470 --> 01:04:32.200
Mani K: yep.

364
01:04:32.850 --> 01:04:33.939
Matt Lee: Okay. Thank you.

365
01:04:34.563 --> 01:04:42.400
Mani K: Okay, so how do we? How? How can you contact me? I think, I did add my email in the

366
01:04:42.500 --> 01:05:05.649
Mani K: slide deck, I think last time. So yeah, so this would be the best way to contact me. Or or you can obviously file a support ticket as well. if you really want to reach out to me through support ticket, you can. Just I'll put a comment there with my name, so I think they'll they'll just route it to me. Okay, so either email or through support tickets, anything should work out fine.

367
01:05:06.350 --> 01:05:14.660
Mani K: Alright, thanks, everyone. And we'll probably connect again. Or another officer session in a few weeks.

368
01:05:15.210 --> 01:05:16.090
Mani K: Bye-bye.

369
01:05:16.090 --> 01:05:16.803
Vivian : Thank you.

