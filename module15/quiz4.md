# Linear Regression Quiz

## Question 1 (1 point)
Consider the following two-dimensional linear regression model:

```
fğœƒ(x) = (x)áµ€Î¸ = Î¸â‚€xâ‚€ + Î¸â‚xâ‚
```

What would the squared loss be for the single prediction of a linear regression model provided below?

l(Î¸,x,yáµ¢)

**Options:**
1. (Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â²
2. (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)
3. (yáµ¢âˆ’Î¸â‚€xâ‚€)Â²
4. (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â²

**Answer:** Option 4 - (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â²

**Explanation:**
Core Definition: The squared loss function measures the squared difference between the actual value (yáµ¢) and the predicted value (fğœƒ(x)).

Implementation Details:
- The predicted value is fğœƒ(x) = Î¸â‚€xâ‚€ + Î¸â‚xâ‚
- We subtract this prediction from the actual value yáµ¢
- The difference is squared to ensure positive values and penalize larger errors more heavily
- Options 1-3 are incorrect because they either miss components or don't square the difference

---

## Question 2 (1 point)
The squared loss function for a two-dimensional regression model is given by:

```
l(Î¸,x,yáµ¢) = (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â²
```

What would the gradient formula of this loss function be?

**Options:**
1. [-2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚€)]
2. [-2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)âˆ’2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)]
3. [-2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚€)âˆ’2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)]
4. [-2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)]

**Answer:** Option 3

**Explanation:**
Core Definition: The gradient is a vector of partial derivatives with respect to each parameter (Î¸â‚€ and Î¸â‚).

Implementation Details:
- Using the chain rule to differentiate (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â²:
  - âˆ‚/âˆ‚Î¸â‚€: -2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚€)
  - âˆ‚/âˆ‚Î¸â‚: -2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)
- The gradient combines both partial derivatives into a vector
- Options 1 and 4 are incomplete as they only include one component
- Option 2 incorrectly repeats the Î¸â‚ component

---

## Question 3 (1 point)
The loss function for a two-dimensional linear regression model is defined as:

```
L(Î¸â‚,Î¸â‚‚) = 1/n âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’(Î¸â‚xáµ¢+Î¸â‚‚))Â²
```

What is the partial derivative of L with respect to Î¸â‚?

**Options:**
1. âˆ’1/n âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’Î¸â‚xáµ¢)
2. âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’(Î¸â‚xáµ¢+Î¸â‚‚))
3. 2/n âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’(Î¸â‚xáµ¢+Î¸â‚‚))â‹…xâ‚
4. âˆ’2/n âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’(Î¸â‚xáµ¢+Î¸â‚‚))â‹…xâ‚

**Answer:** Option 4

**Explanation:**
Core Definition: The partial derivative measures the rate of change of the loss function with respect to Î¸â‚.

Implementation Details:
- Apply the chain rule to differentiate the squared term
- The 1/n term remains as a coefficient
- The negative sign comes from the negative term inside the squared expression
- The xáµ¢ term comes from differentiating Î¸â‚xáµ¢
- Options 1-3 are incorrect because they either:
  - Miss the factor of 2 from differentiating the square
  - Have the wrong sign
  - Don't include all necessary terms

---

# Answer Key
1. Option 4: (yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)Â² - Squared loss between actual and predicted values
2. Option 3: [-2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚€)âˆ’2(yáµ¢âˆ’Î¸â‚€xâ‚€âˆ’Î¸â‚xâ‚)(xâ‚)] - Complete gradient vector
3. Option 4: âˆ’2/n âˆ‘áµ¢â‚Œâ‚â¿(yáµ¢âˆ’(Î¸â‚xáµ¢+Î¸â‚‚))â‹…xâ‚ - Partial derivative with respect to Î¸â‚