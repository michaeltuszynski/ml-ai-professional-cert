{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d164420705a4fece",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Colab Activity 20.2: Implementing the AdaBoost Algorithm\n",
    "\n",
    "**Time: 60 minutes**\n",
    "\n",
    "This activity focuses on using the `AdaBoostClassifier` and the performance resulting from changing the base classifier that is used.  As discussed in the lectures, adaptive boosting is a successive reweighting of data using a set number of estimators.  These weighted estimators are what form the ensemble, and the predictions are a result of a weighted combination of the estimators.  \n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fetal.zip', compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('fetal_health', axis = 1).values\n",
    "y = df['fetal_health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6037217b547e73a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### `AdaBoostClassifier`\n",
    "\n",
    "\n",
    "\n",
    "Instantiate an `AdaBoostClassifier` estimator with `max_depth=1` and assign it to `ans1` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-610a6433eeb011fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ans1 = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "ans1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9374e15cda778d0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Fitting the Ensemble\n",
    "\n",
    "\n",
    "Define an `AdaBoostClassifier` estimator with default parameters and to fit to the data `X_train` and `y_train`. Assign this model to `model_1` below.\n",
    "\n",
    "Assign the accuracy of the model on the test data to `model_1_acc` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92ed88043191a639",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.881578947368421\n"
     ]
    }
   ],
   "source": [
    "model_1 = AdaBoostClassifier(random_state=42)\n",
    "model_1.fit(X_train, y_train)\n",
    "model_1_acc = model_1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(model_1_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-905335749b0f9e64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Grid Searching the Ensemble\n",
    "\n",
    "\n",
    "As the documentation states [on this page](https://scikit-learn.org/stable/modules/ensemble.html#usage), the main parameters to search are the number of estimators and the complexity of the base estimator.  \n",
    "\n",
    "In the code cell below, create a parameter grid that considers the following parameters:\n",
    "\n",
    "- *number of estimators*: 100, 200\n",
    "- *max_depths*: 1, 2, 3\n",
    "\n",
    "Name this grid `params`.\n",
    "\n",
    "Next, use the grid `params` with the `AdaBoostClassifier` to perform a grid search named `tree_grid` on the train data.  For this step, be sure to set the `random_state = 42` in your `AdaBoostClassifier`. \n",
    "\n",
    "Finally, calculate the score on the test data as `grid_acc`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0510fdbbf90c3779",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266917293233082\n"
     ]
    }
   ],
   "source": [
    "# Problem 3\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'base_estimator': [DecisionTreeClassifier(max_depth=depth) for depth in [1, 2, 3]]\n",
    "}\n",
    "\n",
    "tree_grid = GridSearchCV(\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    params\n",
    ")\n",
    "tree_grid.fit(X_train, y_train)\n",
    "grid_acc = tree_grid.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(grid_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8aa3fa345d098088",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### A Different Base Estimator\n",
    "\n",
    "\n",
    "Consider using a different base estimator such as `LogisticRegression` estimator.  Explore the neighbors parameters with \n",
    "\n",
    "- `C = [.001, 0.01, 0.1, 1.0, 10.0]`\n",
    "\n",
    "Create a `Pipeline` that scales the data first and then implements an `AdaBoostClassifier` with `random_state = 42` and a Logistic Regression model.  Grid search the pipeline with a grid and assign the score on the test data to `score2`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bbf690915a58f45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9078947368421053\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ada', AdaBoostClassifier(\n",
    "        base_estimator=LogisticRegression(),\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "params2 = {\n",
    "    'ada__base_estimator__C': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "}\n",
    "grid2 = GridSearchCV(pipe, params2)\n",
    "grid2.fit(X_train, y_train)\n",
    "score2 = grid2.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(score2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db5a64b691d49358",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Evaluating the models\n",
    "\n",
    "\n",
    "Which model performed the best on the test data?\n",
    "\n",
    "- `a`: Base `AdaBoostClassifier`\n",
    "- `b`: Grid Searched Tree Model\n",
    "- `c`: Grid Searched Logistic Model\n",
    "- `d`: None of the above\n",
    "\n",
    "Assign your answer as a string to `ans5` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12e99140e358ffcc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = {\n",
    "    'a': model_1_acc,\n",
    "    'b': grid_acc,\n",
    "    'c': score2\n",
    "}\n",
    "ans5 = max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary: Implementing AdaBoost Algorithm\n",
    "\n",
    "## Overview\n",
    "This notebook explored the implementation and optimization of the AdaBoost algorithm using scikit-learn, focusing on different base estimators and hyperparameter tuning approaches.\n",
    "\n",
    "## Exercise Progression\n",
    "1. **Basic Setup**: Started with implementing a simple decision tree as base estimator\n",
    "2. **Default AdaBoost**: Implemented basic AdaBoost with default parameters\n",
    "3. **Hyperparameter Tuning**: Explored grid search over number of estimators and tree depth\n",
    "4. **Alternative Base Estimator**: Implemented AdaBoost with Logistic Regression\n",
    "5. **Model Comparison**: Evaluated performance of different implementations\n",
    "\n",
    "## Key Technical Insights\n",
    "- AdaBoost can work with different base estimators (Decision Trees, Logistic Regression)\n",
    "- Key parameters to tune:\n",
    "  - Number of estimators (100, 200)\n",
    "  - Base estimator complexity (tree depth: 1, 2, 3)\n",
    "  - Base estimator parameters (C in Logistic Regression)\n",
    "- Proper pipeline construction is crucial when using scaled data\n",
    "\n",
    "## Implementation Takeaways\n",
    "1. Base estimators must be explicitly instantiated when grid searching their parameters\n",
    "2. Pipeline structure is important when combining scaling with AdaBoost\n",
    "3. Parameter naming in grid search must account for nested estimator structure\n",
    "4. Model performance can vary significantly based on base estimator choice and parameters\n",
    "\n",
    "## Best Practices Demonstrated\n",
    "- Using StandardScaler in pipeline with complex models\n",
    "- Systematic hyperparameter tuning through GridSearchCV\n",
    "- Proper model evaluation using train/test split\n",
    "- Structured approach to model comparison"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "codio_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
